import os, sys, json, shutil, subprocess
from dataclasses import dataclass
from typing import Optional, List, Tuple
from pathlib import Path

import numpy as np
import pandas as pd
import librosa
import torch

# ì™¸ë¶€ ëª¨ë“ˆ(ë¶„ë¦¬í•œ íŒŒì¼)ì—ì„œ ê°€ì ¸ì˜¤ê¸°
from emotion_model import EmotionFrames, emotion_frame_probs
from pitch_model import PitchFrames, pitch_soft_probs_with_speakers

# ===================== ì‚¬ìš©ì ì„¤ì • =====================
INPUT_AUDIO = r"C:\Users\user\PycharmProjects\emotion_subtitle_improve\sample.wav"
OUTPUT_DIR  = r"C:\Users\user\PycharmProjects\emotion_subtitle_improve\sample_test_1-2"
LANG        = "en"        # whisper ë‹¨ì–´ ì¶”ì¶œ ê°•ì œ ì–¸ì–´ ("ko" ê°€ëŠ¥)
WORDS_CSV   = None        # ë¯¸ì‚¬ìš© ì‹œ None, ì‚¬ìš© ì‹œ (start,end,word) words.raw.csv ìƒì„±

USE_WHISPER          = True # Whisper ì‚¬ìš© ì—¬ë¶€
USE_DIARIZATION      = True # í™”ì ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€(pynnote)
USE_VOCAL_SEPARATION = True # ë³´ì»¬ ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€(Demusc)

# í™”ì ë¼ë²¨ë§ ë°©ì‹ í† ê¸€
# True  -> ì‹œê°„ íë¦„ì—ì„œ í™”ìê°€ ë°”ë€” ë•Œë§ˆë‹¤ 0,1,2,3... (ë™ì¼ í™”ì ì¬ë“±ì¥ë„ ìƒˆ ë²ˆí˜¸)
# False -> pyannoteì˜ í´ëŸ¬ìŠ¤í„° ë¼ë²¨(SPEAKER_00 ë“±)ì„ ë³´ì¡´ -> ì²˜ìŒ ë°©ì‹
SEQUENTIAL_SPEAKER_LABELS = True

# ì •í™•ë„ 91% ê°ì • ëª¨ë¸
EMO_MODEL_ID         = "firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3"
# í™”ì ë¶„ë¦¬ ëª¨ë¸
DIARIZATION_MODEL_ID = "pyannote/speaker-diarization-3.1"

# í”„ë ˆì´ë°
WIN_S = 0.5     # í”„ë ˆì„ ìœˆë„ìš° í¬ê¸°(ì´ˆ ë‹¨ìœ„)
HOP_S = 0.25    # í”„ë ˆì„ ì´ë™ ê°„ê²©(ì´ˆ ë‹¨ìœ„)

# í”¼ì¹˜ ì†Œí”„íŠ¸ ë¶„ë¥˜(ì„¸ë¯¸í†¤)
PITCH_DELTA_ST = 2.0    # ê¸°ì¤€ ìŒì •ì—ì„œ ëª‡ ì„¸ë¯¸í†¤ ë–¨ì–´ì¡ŒëŠ”ì§€ë¥¼ low/mid/highë¡œ êµ¬ë¶„í• ì§€ ê¸°ì¤€
PITCH_SIGMA    = 1.2    # í™•ë¥  ë¶„í¬ ê³„ì‚° ì‹œ ê°€ìš°ì‹œì•ˆ ë¶„ì‚° ê°’

# pyannote diarization ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ HuggingFace API í† í°
HF_TOKEN = "HF_TOKEN_REDACTED"
# ======================================================

# ---------------------------
# ìœ í‹¸ / IO
# ---------------------------
# ì§€ì •í•œ ê²½ë¡œ pì— ë””ë ‰í† ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ìƒì„±
def safe_mkdir(p: str):
    os.makedirs(p, exist_ok=True)

# CSV íŒŒì¼ ì €ì¥ í•¨ìˆ˜
def write_csv(df: pd.DataFrame, path: str):
    df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ Saved: {path}")

# librosaë¥¼ ì´ìš©í•´ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
def load_audio(path: str):
    y, sr = librosa.load(path, sr=None, mono=True)
    return y, sr

# ì½˜ì†”ì— ê°„ë‹¨íˆ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥ í•¨ìˆ˜
def preview(df: pd.DataFrame, name: str, n: int = 10):
    try:
        print(f"ğŸ“ {name} ë¯¸ë¦¬ë³´ê¸° (top {n})")
        print(df.head(n).to_string(index=False))
    except Exception as e:
        print(f"(ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e})")

# ---------------------------
# ë³´ì»¬ ë¶„ë¦¬
# ---------------------------
def separate_vocals_with_demucs(wav_path: str, session_dir: str) -> str:
    # ì…ë ¥ ì˜¤ë””ì˜¤ ê²½ë¡œì™€ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë°˜í™˜
    wav_path    = str(Path(wav_path).resolve())
    session_dir = Path(session_dir).resolve()
    # ë¶„ë¦¬ ê²°ê³¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
    out_root    = session_dir / "separation" / "demucs_out"
    out_root.mkdir(parents=True, exist_ok=True)

    # GPUê°€ ìˆìœ¼ë©´ cuda, ì—†ìœ¼ë©´ cpu ì‚¬ìš©
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # Demucs ì‹¤í–‰ ëª…ë ¹ì–´ êµ¬ì„±
    cmd = [
        sys.executable, "-m", "demucs.separate",
        "-n", "htdemucs",
        "--two-stems", "vocals",
        "-d", device,
        "-o", str(out_root),
        wav_path
    ]
    # subprocessë¡œ ì™¸ë¶€ ëª…ë ¹ì–´ ì‹¤í–‰
    proc = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore")
    # ë§Œì•½ demucs ì‹¤í–‰ ì‹¤íŒ¨ -> ì›ë³¸ ì˜¤ë””ì˜¤ ê²½ë¡œ ë°˜í™˜
    if proc.returncode != 0:
        print("âŒ Demucs ì‹¤íŒ¨ â€” ì›ë³¸ìœ¼ë¡œ ì§„í–‰")
        return wav_path

    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ì•ˆì—ì„œ 'vocals.wav' íŒŒì¼ ì°¾ê¸°
    candidates = list(out_root.rglob("vocals.wav"))
    if not candidates: # ê²°ê³¼ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        print("âŒ Demucs ì¶œë ¥ ì—†ìŒ â€” ì›ë³¸ìœ¼ë¡œ ì§„í–‰")
        return wav_path

    demucs_vocals = candidates[0]
    fixed = session_dir / "separation" / "vocals.wav"
    fixed.parent.mkdir(parents=True, exist_ok=True)
    shutil.copyfile(demucs_vocals, fixed)

    # ìµœì¢…ì ìœ¼ë¡œ ë³´ì»¬ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
    return str(fixed)

# ---------------------------
# Whisper ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
# ---------------------------
def whisper_word_timestamps(audio_path: str, language: Optional[str] = "en") -> pd.DataFrame:
    import whisper  # pip install openai-whisper
    print("ğŸ”¤ Whisper(small) ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ...")
    # whisper small ëª¨ë¸ ë¡œë“œ
    model = whisper.load_model("small")
    # ì˜¤ë””ì˜¤ íŒŒì¼ì„ Whisperë¡œ ë³€í™˜
    result = model.transcribe(audio_path, language=language, word_timestamps=True, verbose=False)
    words = [] # ë‹¨ì–´ë³„ ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    for seg in result.get("segments", []):
        for w in seg.get("words", []):
            words.append({
                "start": float(w["start"]),  # ë‹¨ì–´ ì‹œì‘ ì‹œê°„
                "end": float(w["end"]),      # ë‹¨ì–´ ë ì‹œê°„
                "word": w["word"].strip()    # ë‹¨ì–´ í…ìŠ¤íŠ¸
            })
    df = pd.DataFrame(words, columns=["start","end","word"])
    if df.empty:
        raise RuntimeError("Whisperê°€ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return df

# ---------------------------
# í™”ì ë¶„ë¦¬(ë‹¤ì´ì–´ë¼ì´ì œì´ì…˜)
# ---------------------------

@dataclass
class DiarizationSeg:
    start: float  # êµ¬ê°„ ì‹œì‘ ì‹œê°„
    end: float    # êµ¬ê°„ ë ì‹œê°„
    speaker: str  # ì›ë³¸ ë¼ë²¨ (SPEAKER_00 ë“±)

# pyannoteë¥¼ ì´ìš©í•´ ì˜¤ë””ì˜¤ì—ì„œ í™”ì êµ¬ê°„ ì¶”ì¶œ
def run_diarization(audio_path: str, model_id=DIARIZATION_MODEL_ID) -> List[DiarizationSeg]:
    print("ğŸ‘¥ í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘ (pyannote)...")
    from pyannote.audio import Pipeline

    # HuggingFace í† í°ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜
    if not HF_TOKEN or not isinstance(HF_TOKEN, str):
        raise RuntimeError("HF_TOKENì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ HuggingFace í† í°ì„ ë„£ì–´ì£¼ì„¸ìš”.")

    # pyannote diarization íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    pipeline = Pipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN)
    # ì˜¤ë””ì˜¤ íŒŒì¼ì— ëŒ€í•´ í™”ì ë¶„ë¦¬ ìˆ˜í–‰ -> Annotation ê°ì²´ ë°˜í™˜
    diar = pipeline(audio_path)
    segs: List[DiarizationSeg] = []
    for turn, _, speaker in diar.itertracks(yield_label=True):
        segs.append(DiarizationSeg(start=float(turn.start), end=float(turn.end), speaker=str(speaker)))
    # ì‹œì‘ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    segs.sort(key=lambda s: s.start)
    # ì „ì²´ í™”ì ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    uniq = sorted({s.speaker for s in segs})
    print(f"ğŸ§ª unique speakers from diarization: {uniq} (count={len(uniq)})")
    print(f"ğŸ‘¥ í™”ì ì„¸ê·¸ë¨¼íŠ¸ {len(segs)}ê°œ")
    return segs

# DiarizationSeg ë¦¬ìŠ¤íŠ¸ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜
def diar_to_dataframe(segs: List[DiarizationSeg]) -> pd.DataFrame:
    return pd.DataFrame([{"start": s.start, "end": s.end, "speaker": s.speaker} for s in segs])

# í™”ì ë¼ë²¨ì„ "ì‹œê°„ ìˆœì°¨ ë¼ë²¨"ë¡œ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜ -> ìƒˆë¡œìš´ ê°œì„  ë°©ë²•
def _assign_label_seq_over_time(diar_df: pd.DataFrame) -> pd.DataFrame:
    """
    ì‹œê°„ íë¦„ì—ì„œ 'í™”ìê°€ ë°”ë€” ë•Œë§ˆë‹¤' 0,1,2,3...ë¥¼ ë¶€ì—¬.
    ë™ì¼ í™”ìê°€ ë‹¤ì‹œ ë“±ì¥í•´ë„ ìƒˆ ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•˜ëŠ” ê·œì¹™.
    """
    if diar_df.empty:
        return diar_df.copy()
    # ì‹œê°„ ê¸°ì¤€ ì •ë ¬
    diar_df = diar_df.sort_values(["start", "end"]).reset_index(drop=True)
    seq_labels = []
    last_raw = None
    counter = 0
    # ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ìˆœíšŒ
    for _, row in diar_df.iterrows():
        raw = row["speaker"]
        # ì§ì „ í™”ìì™€ ë‹¤ë¥´ë©´ ìƒˆë¡œìš´ ë²ˆí˜¸ ë¶€ì—¬
        if raw != last_raw:
            label = str(counter)
            counter += 1
            last_raw = raw
        seq_labels.append(label)
    # speaker_seq ì»¬ëŸ¼ ì¶”ê°€
    out = diar_df.copy()
    out["speaker_seq"] = seq_labels
    return out

# ì‹¤ì œë¡œ downstreamì—ì„œ ì‚¬ìš©í•  speaker ì»¬ëŸ¼ ë·° ìƒì„±
def make_active_diar_view(diar_df: pd.DataFrame, use_sequential: bool) -> pd.DataFrame:
    """
    downstreamì—ì„œ ì°¸ì¡°í•  í†µì¼ëœ ì»¬ëŸ¼(speaker)ì„ ìƒì„±í•´ ë°˜í™˜.
    - use_sequential=True  -> speaker <- speaker_seq
    - use_sequential=False -> speaker ê·¸ëŒ€ë¡œ
    """
    if diar_df.empty:
        return diar_df.copy()
    if use_sequential:
        if "speaker_seq" not in diar_df.columns:
            diar_df = _assign_label_seq_over_time(diar_df)
        # speaker_seqë¥¼ speakerë¡œ ë°”ê¿”ì„œ ë°˜í™˜
        view = diar_df[["start", "end", "speaker_seq"]].rename(columns={"speaker_seq": "speaker"}).copy()
    else:
        # ì›ë³¸ ë¼ë²¨ ê·¸ëŒ€ë¡œ ë°˜í™˜
        view = diar_df[["start", "end", "speaker"]].copy()
    return view

# íŠ¹ì • ì‹œê° tì—ì„œ í™œì„± í™”ì ë¼ë²¨ ì°¾ê¸°
def assign_speaker_at_time(t: float, diar_view: pd.DataFrame, default="unknown") -> str:
    # tê°€ ì†í•œ í™”ì êµ¬ê°„ ì°¾ê¸°
    hits = diar_view[(diar_view["start"] <= t) & (diar_view["end"] >= t)]
    if hits.empty:
        return default
    # ê²¹ì¹˜ëŠ” êµ¬ê°„ì´ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°, ê¸¸ì´ê°€ ê°€ì¥ ê¸´ êµ¬ê°„ ì„ íƒ
    lens = (hits["end"] - hits["start"]).to_numpy()
    return str(hits.iloc[int(lens.argmax())]["speaker"])

# í”„ë ˆì„ ë‹¨ìœ„ë¡œ í™”ì ë¼ë²¨ ë§¤í•‘
def assign_speaker_to_frames(frame_times: np.ndarray, diar_view: pd.DataFrame) -> List[str]:
    return [assign_speaker_at_time(float(t), diar_view) for t in frame_times]

# ë‹¨ì–´ë³„ë¡œ í™”ì ë¼ë²¨ ë¶€ì—¬
def annotate_words_with_speaker(words_df: pd.DataFrame, diar_view: pd.DataFrame) -> pd.DataFrame:
    speakers = []
    for _, w in words_df.iterrows():
        # ë‹¨ì–´ì˜ ì¤‘ê°„ ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ í™”ì ì°¾ê¸°
        mid = (float(w["start"]) + float(w["end"])) / 2.0
        speakers.append(assign_speaker_at_time(mid, diar_view))
    out = words_df.copy()
    out["speaker"] = speakers
    return out

# ---------------------------
# ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„ â€” í™”ìê°’ 'ê·¸ëŒ€ë¡œ' ë³´ì¡´(ìˆœì°¨ ë¼ë²¨)
# ---------------------------
def aggregate_over_words(words_seq_df: pd.DataFrame,
                         emo: EmotionFrames,
                         pitch: PitchFrames,
                         pad_s: float = 0.05) -> pd.DataFrame:
    out_rows = []
    emo_labels = emo.labels if emo.labels else [f"class_{i}" for i in range(emo.probs.shape[1])]
    pitch_labels = ["low", "mid", "high"]

    etimes, eprobs, erms = emo.times, emo.probs, emo.rms
    ptimes, pprobs = pitch.times, pitch.probs

    for _, w in words_seq_df.iterrows():
        t0 = float(w["start"]) - pad_s
        t1 = float(w["end"]) + pad_s

        # ğŸ”’ í™”ì: ìˆœì°¨ ë¼ë²¨ ì»¬ëŸ¼ speakerë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        speaker_seq = str(w["speaker"]) if "speaker" in w and pd.notna(w["speaker"]) else "unknown"

        # ê°ì • ì§‘ê³„ (RMS ê°€ì¤‘ í‰ê· )
        if etimes.size:
            mask_e = (etimes >= t0) & (etimes <= t1)
            Pe = eprobs[mask_e]
            We = erms[mask_e]
            if Pe.shape[0] > 0:
                if We.sum() <= 1e-12:
                    We = np.ones_like(We) / len(We)
                else:
                    We = We / We.sum()
                emo_mean = (Pe * We[:, None]).sum(axis=0)
                emo_entropy = float(-(emo_mean * np.log(emo_mean + 1e-9)).sum())
                emo_top = emo_labels[int(np.argmax(emo_mean))]
                emo_probs_dict = {emo_labels[i]: float(emo_mean[i]) for i in range(len(emo_mean))}
            else:
                emo_entropy = float(np.nan); emo_top = ""; emo_probs_dict = {}
        else:
            emo_entropy = float(np.nan); emo_top = ""; emo_probs_dict = {}

        # í”¼ì¹˜ ì§‘ê³„ (í‰ê· )
        if ptimes.size:
            mask_p = (ptimes >= t0) & (ptimes <= t1)
            Pp = pprobs[mask_p]
            if Pp.shape[0] > 0:
                Wp = np.ones((Pp.shape[0],), dtype=float) / Pp.shape[0]
                pitch_mean = (Pp * Wp[:, None]).sum(axis=0)
                pitch_entropy = float(-(pitch_mean * np.log(pitch_mean + 1e-9)).sum())
                pitch_top = pitch_labels[int(np.argmax(pitch_mean))]
                pitch_probs_dict = {pitch_labels[i]: float(pitch_mean[i]) for i in range(3)}
            else:
                pitch_entropy = float(np.nan); pitch_top = ""; pitch_probs_dict = {}
        else:
            pitch_entropy = float(np.nan); pitch_top = ""; pitch_probs_dict = {}

        out_rows.append({
            "start": float(w["start"]),
            "end": float(w["end"]),
            "word": str(w["word"]),
            "speaker": speaker_seq,                      # â† ìˆœì°¨ ë¼ë²¨
            "emo_label": emo_top,
            "emo_entropy": emo_entropy,
            "emo_probs": json.dumps(emo_probs_dict, ensure_ascii=False),
            "pitch_label": pitch_top,
            "pitch_entropy": pitch_entropy,
            "pitch_probs": json.dumps(pitch_probs_dict, ensure_ascii=False),
        })

    return pd.DataFrame(out_rows)

# ---------------------------
# ë©”ì¸
# ---------------------------
def main():
    safe_mkdir(OUTPUT_DIR)

    # 1) ì…ë ¥ ë¡œë“œ & ë³´ì»¬ ë¶„ë¦¬
    src_wav = INPUT_AUDIO
    print(f"ğŸµ Input: {src_wav}")
    use_path = separate_vocals_with_demucs(src_wav, OUTPUT_DIR) if USE_VOCAL_SEPARATION else src_wav
    print(f"ğŸ¤ ë¶„ì„ ì˜¤ë””ì˜¤: {use_path}")

    # 2) ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
    if USE_WHISPER:
        try:
            words_df = whisper_word_timestamps(use_path, language=LANG)
        except Exception as e:
            print(f"âš ï¸ Whisper ì‹¤íŒ¨: {e}")
            if WORDS_CSV and os.path.exists(WORDS_CSV):
                words_df = pd.read_csv(WORDS_CSV)
                assert {"start","end","word"}.issubset(words_df.columns)
            else:
                raise
    else:
        if WORDS_CSV and os.path.exists(WORDS_CSV):
            words_df = pd.read_csv(WORDS_CSV)
            assert {"start","end","word"}.issubset(words_df.columns)
        else:
            raise RuntimeError("USE_WHISPER=False ì¸ ê²½ìš° WORDS_CSV ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    words_raw_csv = os.path.join(OUTPUT_DIR, "words_raw.csv")
    write_csv(words_df, words_raw_csv)
    preview(words_df, "Whisper ë‹¨ì–´")
    print(f"ğŸ“„ ë‹¨ì–´ íŒŒì¼ ì €ì¥: {words_raw_csv}")

    # 3) í™”ì ë¶„ë¦¬ -> "í™œì„± ë‹¤ì´ì–´ë¦¬ë·°" ìƒì„± (ìˆœì°¨ ë ˆì´ë¸” ëª¨ë“œì¼ ê²½ìš° speaker_seq ì‚¬ìš©)
    diar_df = pd.DataFrame(columns=["start","end","speaker"])
    diar_view = pd.DataFrame(columns=["start","end","speaker"])
    if USE_DIARIZATION:
        try:
            segs = run_diarization(use_path, DIARIZATION_MODEL_ID)
            diar_df = diar_to_dataframe(segs)
            # ìˆœì°¨ ë ˆì´ë¸” ì»¬ëŸ¼ ì¶”ê°€
            diar_df = _assign_label_seq_over_time(diar_df)
            diar_csv = os.path.join(OUTPUT_DIR, "diarization_segments.csv")
            write_csv(diar_df, diar_csv)

            # ğŸ’¡ downstreamì—ì„œ ì°¸ì¡°í•  'speaker' ë·° í™•ì •
            diar_view = make_active_diar_view(diar_df, use_sequential=SEQUENTIAL_SPEAKER_LABELS)

            # ë‹¨ì–´ì— í™”ì ë¶€ì—¬ (í™œì„± ë·° ê¸°ì¤€)
            words_df = annotate_words_with_speaker(words_df, diar_view)
        except Exception as e:
            print(f"âš ï¸ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")
            diar_df = pd.DataFrame(columns=["start","end","speaker"])
            diar_view = diar_df.copy()
            words_df["speaker"] = "unknown"
    else:
        words_df["speaker"] = "unknown"

    # ğŸ”’ words_dfì— ê¸°ë¡ëœ speaker ë¥¼ ê³ ì •(ì´ ê°’ì´ words_with_speaker.csv & ìµœì¢… CSVì— ê·¸ëŒ€ë¡œ ì‚¬ìš©ë¨)
    words_df = words_df.sort_values(["start","end"]).reset_index(drop=True)

    # words_with_speaker.csv ì €ì¥
    words_spk_csv = os.path.join(OUTPUT_DIR, "words_with_speaker.csv")
    write_csv(words_df, words_spk_csv)
    preview(words_df, "ë‹¨ì–´+í™”ì ë¼ë²¨(ìµœì¢… ê³ ì •)")
    print(f"ğŸ“„ ë‹¨ì–´+í™”ì íŒŒì¼ ì €ì¥: {words_spk_csv}")

    # 4) ì˜¤ë””ì˜¤ ë¡œë“œ (ê°ì •/í”¼ì¹˜ ì¶”ë¡  ì¤€ë¹„)
    y, sr = load_audio(use_path)

    # 5) ê°ì • í”„ë ˆì„ ì¶”ë¡  (ê¸°ì¡´ ë¡œì§)
    print("ğŸ§  Emotion frame inference...")
    emo = emotion_frame_probs(y, sr, EMO_MODEL_ID)

    # 6) í”¼ì¹˜: âœ… "í™œì„± ë‹¤ì´ì–´ë¦¬ë·°(speaker=ìˆœì°¨ ë ˆì´ë¸”)" ê¸°ì¤€ìœ¼ë¡œ í™”ìë³„ F0 ì¤‘ì•™ê°’ ê³„ì‚°
    print("ğŸ¼ Pitch soft-prob (speaker-aware, sequential labels if enabled)...")
    pitch = pitch_soft_probs_with_speakers(y, sr, diar_view, WIN_S, HOP_S, PITCH_DELTA_ST, PITCH_SIGMA)

    # 7) ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„ (words_dfì˜ speaker ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    print("ğŸ§® Aggregating per word...")
    out_df = aggregate_over_words(words_df, emo, pitch)

    # 8) ì €ì¥
    out_csv = os.path.join(OUTPUT_DIR, "words_emotion_pitch.csv")
    write_csv(out_df, out_csv)
    print("âœ… Done.")

if __name__ == "__main__":
    main()
