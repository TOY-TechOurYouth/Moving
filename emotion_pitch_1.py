# filename: full_emotion_pitch_with_diarization.py
# --------------------------------------------------
# ì…ë ¥ ì˜¤ë””ì˜¤ -> (ì„ íƒ)ë³´ì»¬ ë¶„ë¦¬ -> Whisper ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
#           -> (ì„ íƒ)í™”ì ë¶„ë¦¬(pyannote) -> í”„ë ˆì„ ê°ì • í™•ë¥ (W2V2 SER)
#           -> í”¼ì¹˜(F0->ì„¸ë¯¸í†¤->ì†Œí”„íŠ¸ í™•ë¥ , í™”ìë³„ ê¸°ì¤€ì¹˜) -> ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„
#           -> CSV ì €ì¥(start, end, word, speaker, emo_*, pitch_*)
# --------------------------------------------------
import sys

INPUT_AUDIO = r"C:\Users\user\PycharmProjects\emotion_subtitle_improve\sample.wav"   # â† ì…ë ¥ ì˜¤ë””ì˜¤ ê²½ë¡œ
OUTPUT_DIR  = r"C:\Users\user\PycharmProjects\emotion_subtitle_improve\sample_test_1"      # â† ì¶œë ¥ í´ë”
LANG        = "en"                           # Whisper ê°•ì œ ì–¸ì–´ (ì˜ˆ: "en", "ko")
WORDS_CSV   = None                           # Whisper ëŒ€ì‹  ì“¸ ë‹¨ì–´ CSV(start,end,word). ì—†ìœ¼ë©´ None

USE_VOCAL_SEPARATION = True                  # Spleeter/Demucsê°€ ìˆìœ¼ë©´ ë³´ì»¬ ë¶„ë¦¬ ì‚¬ìš©
USE_WHISPER          = True                  # openai-whisper ì„¤ì¹˜ ì‹œ ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ìë™ ì¶”ì¶œ
USE_DIARIZATION      = True                  # pyannote.audio ì„¤ì¹˜ + HF_TOKEN í•„ìš”

# ëª¨ë¸ ID
EMO_MODEL_ID          = "firdhokk/speech-emotion-recognition"         # 7-class SER
DIARIZATION_MODEL_ID  = "pyannote/speaker-diarization-3.1"            # pyannote diarization pipeline

# í”„ë ˆì´ë° íŒŒë¼ë¯¸í„° (ê°ì •/í”¼ì¹˜ ê³µí†µ ê·¸ë¦¬ë“œ)
WIN_S = 0.5        # ìœˆë„ ê¸¸ì´(ì´ˆ)
HOP_S = 0.25       # í™‰(ì´ˆ)
WORD_PAD_S = 0.05  # ë‹¨ì–´ ê²½ê³„ íŒ¨ë”©(Â±ì´ˆ)

# í”¼ì¹˜ ì†Œí”„íŠ¸ ë¶„ë¥˜(ì„¸ë¯¸í†¤) íŒŒë¼ë¯¸í„°
PITCH_DELTA_ST = 2.0   # class centers [-Î”, 0, +Î”] st
PITCH_SIGMA    = 1.2   # Gaussian sigma (st)

HF_TOKEN = "HF_TOKEN_REDACTED"  # â† ì—¬ê¸°ì— í† í° ì…ë ¥

# ==================================================

import os, json, math
from dataclasses import dataclass
from typing import Optional, Tuple, List
import numpy as np
import pandas as pd
import soundfile as sf
import librosa
import torch
import torch.nn.functional as F
import subprocess, shutil
from pathlib import Path

from transformers import AutoModelForAudioClassification # ì˜¤ë””ì˜¤ ë¶„ë¥˜ìš© ì‚¬ì „í•™ìŠµ ëª¨ë¸
from transformers import AutoFeatureExtractor as AudioProcessorClass # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ê¸°(íŒŒí˜• -> ëª¨ë¸ ì…ë ¥ í…ì„œ)

from pydub import AudioSegment # ì˜¤ë””ì˜¤ í¬ë§· ë³€í™˜/ìë¥´ê¸°/í•©ì¹˜ê¸° ë“±ì— ìœ ìš©
AudioSegment.converter = r"C:\ffmpeg-7.1.1-essentials_build\bin\ffmpeg.exe"

def preview(df: pd.DataFrame, name: str, n: int = 10):
    try:
        print(f"ğŸ“ {name} ë¯¸ë¦¬ë³´ê¸° (top {n})")
        print(df.head(n).to_string(index=False))
    except Exception as e:
        print(f"(ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e})")

# ---------------------------
# ìœ í‹¸
# ---------------------------

# ì•ˆì „í•œ ë””ë ‰í„°ë¦¬ ìƒì„±
def safe_mkdir(p: str):
    os.makedirs(p, exist_ok=True)

# ì˜¤ë””ì˜¤ íŒŒí˜• yì™€ ìƒ˜í”Œë ˆì´íŠ¸ srì„ ë°›ì•„ì„œ ëª¨ë…¸ë¡œ ë³€í™˜ í›„ 16kHzë¡œ ë¦¬ìƒ˜í”Œ
def ensure_mono_16k(y, sr, target_sr=16000):
    # ì±„ë„ ì°¨ì›ì´ 2ì´ë©´ ìŠ¤í…Œë ˆì˜¤ë¡œ ê°„ì£¼í•˜ê³  ëª¨ë…¸ë¡œ ë³€í™˜
    if y.ndim == 2:
        y = librosa.to_mono(y.T)
    # ìƒ˜í”Œë ˆì´íŠ¸ê°€ ëª©í‘œì™€ ë‹¤ë¥´ë©´ ë¦¬ìƒ˜í”Œ
    if sr != target_sr:
        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)
        sr = target_sr
    return y, sr

# CSVë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë¡œê·¸ë¡œ ë‚¨ê¹€
def write_csv(df: pd.DataFrame, path: str):
    df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ Saved: {path}")

# ì˜¤ë””ì˜¤ ë¡œë“œ
def load_audio(path: str):
    y, sr = librosa.load(path, sr=None, mono=True)
    return y, sr

# í”„ë ˆì„ ë¬¶ìŒ(2D ë°°ì—´)ì— ëŒ€í•´ ê° í”„ë ˆì„ì˜ RMSë¥¼ ê³„ì‚° -> ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„ì—ì„œ í™œìš©
def rms_per_frame(frames: np.ndarray) -> np.ndarray:
    return np.sqrt((frames**2).mean(axis=1)) if len(frames) else np.array([])

# ---------------------------
# ë³´ì»¬ ë¶„ë¦¬
# ---------------------------
def separate_vocals_with_demucs(wav_path: str, session_dir: str) -> str:
    """
    ì…ë ¥ WAVì—ì„œ ë³´ì»¬ë§Œ Demucsë¡œ ë¶„ë¦¬í•´ session_dir/separation/vocals.wavë¡œ ì €ì¥.
    ì‹¤íŒ¨í•˜ë©´ ì˜ˆì™¸ ëŒ€ì‹  ì›ë³¸ ê²½ë¡œë¥¼ ë°˜í™˜(í´ë°±)í•˜ë„ë¡ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬.
    """
    from pathlib import Path
    import subprocess, shutil, sys
    import torch

    # ì…ë ¥ ê²½ë¡œ/ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì ˆëŒ€ê²½ë¡œí™”
    wav_path    = str(Path(wav_path).resolve())
    session_dir = Path(session_dir).resolve()
    # ì¶œë ¥ ë£¨íŠ¸
    out_root    = session_dir / "separation" / "demucs_out"
    out_root.mkdir(parents=True, exist_ok=True)

    # GPU ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ ì¥ì¹˜ ìë™ ì„ íƒ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    cmd = [
        sys.executable, "-m", "demucs.separate",
        "-n", "htdemucs",
        "--two-stems", "vocals",
        "-d", device,                # auto(cu/cpu)
        "-o", str(out_root),
        wav_path
    ]
    proc = subprocess.run(
        cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore"
    )

    # ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
    if proc.returncode != 0:
        print("âŒ Demucs ì‹¤íŒ¨")
        print("â”€â”€ stdout â”€â”€")
        print(proc.stdout.strip())
        print("â”€â”€ stderr â”€â”€")
        print(proc.stderr.strip())
        print("âš ï¸ ë³´ì»¬ ë¶„ë¦¬ ì—†ì´ ì›ë³¸ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return wav_path  # í´ë°±

    # ì‹¤í–‰ ì„±ê³µí–ˆì§€ë§Œ vocal.wavë¥¼ ëª» ì°¾ì€ ê²½ìš°
    # ë²„ì „/í”Œë«í¼ë³„ í´ë” ì°¨ì´ë¥¼ í—ˆìš©: ì–´ë””ë“  vocals.wavë§Œ ì°¾ì•„ì˜´
    candidates = list(out_root.rglob("vocals.wav"))
    if not candidates:
        print("âŒ Demucs ì¶œë ¥ íŒŒì¼(vocals.wav)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("â”€â”€ stdout â”€â”€")
        print(proc.stdout.strip())
        print("â”€â”€ stderr â”€â”€")
        print(proc.stderr.strip())
        print("âš ï¸ ë³´ì»¬ ë¶„ë¦¬ ì—†ì´ ì›ë³¸ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return wav_path  # í´ë°±

    # ì •ìƒì ìœ¼ë¡œ ì°¾ì€ ê²½ìš° -> ì²« ë²ˆì§¸ vocals.wav ì‚¬ìš©
    demucs_vocals = candidates[0]
    fixed = session_dir / "separation" / "vocals.wav"
    fixed.parent.mkdir(parents=True, exist_ok=True)
    shutil.copyfile(demucs_vocals, fixed)
    # ìµœì¢… ë°˜í™˜: ë¶„ë¦¬ëœ vocals.wavì˜ ê²½ë¡œ
    return str(fixed)

# ---------------------------
# Whisper ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
# ---------------------------
def whisper_word_timestamps(audio_path: str, language: Optional[str] = "en") -> pd.DataFrame:
    import whisper  # pip install openai-whisper
    print("ğŸ”¤ Whisper(small) ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ...")
    # "tiny", "base", "small", "medium", "large" ë“± í¬ê¸°ë³„ ëª¨ë¸ ì¡´ì¬ -> í¬ê¸° í´ìˆ˜ë¡ ì •í™•ë„ëŠ” ë†’ì•„ì§€ì§€ë§Œ ì†ë„ê°€ ëŠë ¤ì§
    model = whisper.load_model("small")
    result = model.transcribe(audio_path, language=language, word_timestamps=True, verbose=False)
    words = []
    # Whisper ì¶œë ¥ êµ¬ì¡°
    for seg in result.get("segments", []): # result["segments"] = ë¬¸ì¥/êµ¬ê°„ ë‹¨ìœ„ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        for w in seg.get("words", []): # seg["words"] = ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ë‹¨ì–´ë³„ ì •ë³´ (start, end, word)
            words.append({"start": w["start"], "end": w["end"], "word": w["word"].strip()})
    df = pd.DataFrame(words)
    if df.empty:
        raise RuntimeError("Whisperê°€ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return df

# ---------------------------
# í™”ì ë¶„ë¦¬(ë‹¤ì´ì–´ë¼ì´ì œì´ì…˜)
# ---------------------------

# í•˜ë‚˜ì˜ í™”ì êµ¬ê°„ì„ ë‹´ëŠ” ë°ì´í„° êµ¬ì¡° ì •ì˜
@dataclass
class DiarizationSeg:
    start: float
    end: float
    speaker: str

def run_diarization(audio_path: str, model_id=DIARIZATION_MODEL_ID) -> List[DiarizationSeg]:
    """
    pyannote.audio Pipeline ì‚¬ìš©. ì „ì—­ ìƒìˆ˜ HF_TOKEN ì‚¬ìš©.
    ë°˜í™˜: [DiarizationSeg(...)] ë¦¬ìŠ¤íŠ¸
    """
    print("ğŸ‘¥ í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘ (pyannote)...")
    from pyannote.audio import Pipeline

    # Hugging Face í† í° í•„ìˆ˜
    if not HF_TOKEN or not isinstance(HF_TOKEN, str):
        raise RuntimeError("HF_TOKENì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ HuggingFace í† í°ì„ ë„£ì–´ì£¼ì„¸ìš”.")

    # í™˜ê²½ë³€ìˆ˜ ëŒ€ì‹  í•˜ë“œì½”ë”©ëœ í† í° ì‚¬ìš©
    pipeline = Pipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN)

    # ì˜¤ë””ì˜¤ì— ëŒ€í•´ diarization ì‹¤í–‰
    diar = pipeline(audio_path)  # Annotation
    segs: List[DiarizationSeg] = []
    for turn, _, speaker in diar.itertracks(yield_label=True):
        segs.append(DiarizationSeg(start=float(turn.start), end=float(turn.end), speaker=str(speaker)))
    # ì‹œì‘ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    segs.sort(key=lambda s: s.start)
    print(f"ğŸ‘¥ í™”ì ì„¸ê·¸ë¨¼íŠ¸ {len(segs)}ê°œ")
    return segs

# í™”ì ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ -> dataFrame ë³€í™˜
def diar_to_dataframe(segs: List[DiarizationSeg]) -> pd.DataFrame:
    return pd.DataFrame([{"start": s.start, "end": s.end, "speaker": s.speaker} for s in segs])

# íŠ¹ì • ì‹œê° tì— í•´ë‹¹í•˜ëŠ” í™”ì IDë¥¼ ë°˜í™˜
def assign_speaker_at_time(t: float, diar_df: pd.DataFrame, default="unknown") -> str:
    # tê°€ í¬í•¨ë˜ëŠ” ì²« ì„¸ê·¸ë¨¼íŠ¸ì˜ speaker ë°˜í™˜ (ê²¹ì¹˜ë©´ ê°€ì¥ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ ìš°ì„ )
    hits = diar_df[(diar_df["start"] <= t) & (diar_df["end"] >= t)]
    if hits.empty:
        return default
    # ê°€ì¥ ê¸´ ì„¸ê·¸ë¨¼íŠ¸
    lens = (hits["end"] - hits["start"]).to_numpy()
    return hits.iloc[int(lens.argmax())]["speaker"]

# ì—¬ëŸ¬ í”„ë ˆì„ ì‹œê° ë°°ì—´ì— ëŒ€í•´ ê° í”„ë ˆì„ì´ ì†í•˜ëŠ” í™”ìë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
def assign_speaker_to_frames(frame_times: np.ndarray, diar_df: pd.DataFrame) -> List[str]:
    return [assign_speaker_at_time(float(t), diar_df) for t in frame_times]

# words_with_speaker.csv
def annotate_words_with_speaker(words_df: pd.DataFrame, diar_df: pd.DataFrame) -> pd.DataFrame:
    """
    ê° ë‹¨ì–´ì— ìŠ¤í”¼ì»¤ ë¼ë²¨ ë¶€ì—¬: ë‹¨ì–´ ì¤‘ì•™ ì‹œê° ê¸°ì¤€ìœ¼ë¡œ ë§¤í•‘(ê°„ë‹¨/ê²¬ê³ )
    """
    speakers = []
    for _, w in words_df.iterrows():
        mid = (float(w["start"]) + float(w["end"])) / 2.0
        speakers.append(assign_speaker_at_time(mid, diar_df))
    out = words_df.copy()
    out["speaker"] = speakers
    return out

# ---------------------------
# ê°ì •: í”„ë ˆì„ ì¸í¼ëŸ°ìŠ¤
# ---------------------------
# í”„ë ˆì„ ë‹¨ìœ„ ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° êµ¬ì¡°
@dataclass
class EmotionFrames:
    times: np.ndarray   # ê° í”„ë ˆì„ì˜ ì¤‘ì•™ ì‹œê°„
    probs: np.ndarray   # ê°ì • í™•ë¥  ë°°ì—´ (í”„ë ˆì„ ìˆ˜ X ê°ì • í´ë˜ìŠ¤ ìˆ˜)
    rms: np.ndarray     # í”„ë ˆì„ë³„ RMS ì—ë„ˆì§€ (ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš© ê°€ëŠ¥)
    labels: List[str]   # ê°ì • í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

# ê¸´ ì˜¤ë””ì˜¤ íŒŒí˜•ì„ ì¼ì •í•œ ê¸¸ì´ë¡œ ì˜ë¼ í”„ë ˆì„ ë§Œë“¦
def frame_audio(y: np.ndarray, sr: int, win_s=WIN_S, hop_s=HOP_S):
    win = int(win_s * sr) # ìœˆë„ ê¸¸ì´ë¥¼ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ë³€í™˜
    hop = int(hop_s * sr) # í™‰ í¬ê¸°ë¥¼ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ë³€í™˜
    frames, centers = [], []
    # 0ë¶€í„° len(y)-win ê¹Œì§€ hopì”© ì¦ê°€í•˜ì—¬ í”„ë ˆì„ ìƒì„±
    for start in range(0, max(1, len(y)-win+1), hop):
        end = start + win
        if end > len(y): break # ì˜¤ë””ì˜¤ ë ë„˜ìœ¼ë©´ ì¤‘ë‹¨
        frames.append(y[start:end])
        # í”„ë ˆì„ì˜ ì¤‘ì•™ ìœ„ì¹˜ë¥¼ ì‹œê°„(ì´ˆ)ë¡œ ì €ì¥
        centers.append((start + end) / 2 / sr)
    # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    frames = np.stack(frames) if frames else np.empty((0,))
    centers = np.array(centers, dtype=float)
    rms = rms_per_frame(frames) # í”„ë ˆì„ë³„ RMS ê³„ì‚°
    return frames, centers, rms

# ì˜¤ë””ì˜¤ íŒŒí˜•ì„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ê° í”„ë ˆì„ë³„ ê°ì • í™•ë¥  ê³„ì‚°
def emotion_frame_probs(y: np.ndarray, sr: int, model_id=EMO_MODEL_ID) -> EmotionFrames:
    # ëª¨ë¸ì— ë§ëŠ” ì „ì²˜ë¦¬ê¸°(processor)ì™€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    proc = AudioProcessorClass.from_pretrained(model_id)
    model = AutoModelForAudioClassification.from_pretrained(model_id)
    model.eval() # í‰ê°€ ëª¨ë“œ (dropout ë“± ë¹„í™œì„±í™”)

    # ì˜¤ë””ì˜¤ë¥¼ ëª¨ë…¸/16kë¡œ ë§ì¶¤ (ëª¨ë¸ ìš”êµ¬ SRë¡œ ë³€í™˜)
    y, sr = ensure_mono_16k(y, sr, target_sr=getattr(proc, "sampling_rate", 16000))
    # ì˜¤ë””ì˜¤ë¥¼ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë¶„í• 
    frames, centers, rms = frame_audio(y, sr, WIN_S, HOP_S)
    # í”„ë ˆì„ì´ ì—†ìœ¼ë©´ ë¹ˆ EmotionFrames ë°˜í™˜
    if frames.size == 0:
        return EmotionFrames(np.array([]), np.zeros((0, model.config.num_labels)), np.array([]), [])

    # ì „ì²˜ë¦¬ê¸°ë¡œ íŒŒí˜• -> ëª¨ë¸ ì…ë ¥ í…ì„œ ë³€í™˜
    inputs = proc(frames.tolist(), sampling_rate=sr, return_tensors="pt", padding=True)
    # ëª¨ë¸ ì¶”ë¡ (no_grad: ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” -> ë©”ëª¨ë¦¬/ì†ë„ ì»¤ì§)
    with torch.no_grad():
        logits = model(**{k: v for k, v in inputs.items() if k in ["input_values", "attention_mask"]}).logits
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥ í™”
        probs = F.softmax(logits, dim=-1).cpu().numpy()
    # id2label ë§¤í•‘ìœ¼ë¡œ í´ë˜ìŠ¤ ì´ë¦„ ì¶”ì¶œ
    labels = [model.config.id2label[i] for i in range(model.config.num_labels)]
    return EmotionFrames(centers, probs, rms, labels)


# ---------------------------
# í”¼ì¹˜: F0 -> ì„¸ë¯¸í†¤ -> ì†Œí”„íŠ¸ í™•ë¥  (í™”ìë³„ ê¸°ì¤€ì¹˜)
# ---------------------------
@dataclass
class PitchFrames:
    times: np.ndarray           # ê° í”„ë ˆì„ì˜ ì¤‘ì•™ ì‹œê°„(ì´ˆ)
    probs: np.ndarray           # (N,3) [low, mid, high]
    entropy: np.ndarray         # ê° í”„ë ˆì„ í™•ë¥ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼(ë¶ˆí™•ì‹¤ì„± ì§€í‘œ)
    f0_med_per_speaker: dict    # í™”ìë³„ F0 ì¤‘ì•™ê°’
    st_values: np.ndarray       # ê° í”„ë ˆì„ì˜ ì„¸ë¯¸í†¤ ê°’(ê¸°ì¤€ì¹˜ ëŒ€ë¹„ ìƒëŒ€ í”¼ì¹˜)
    frame_speakers: List[str]   # ê° í”„ë ˆì„ì— ë§¤í•‘ëœ í™”ì ë¼ë²¨

# ê° í”„ë ˆì„ íŒŒí˜•ì— ëŒ€í•´ ê¸°ë³¸ì£¼íŒŒìˆ˜(F0) ì¶”ì • í›„, ì¤‘ì•™ê°’ ê³„ì‚°
def estimate_f0_per_frame(frames: np.ndarray, sr: int) -> np.ndarray:
    """ê° í”„ë ˆì„ì—ì„œ pyinìœ¼ë¡œ F0 ëŒ€í‘œê°’(ì¤‘ì•™ê°’) ì¶”ì •"""
    f0_all = []
    for f in frames:
        try:
            # pyinì€ í”„ë ˆì„ ë‚´ë¶€ë¥¼ ë‹¤ì‹œ ì„¸ë¶€ í”„ë ˆì„ìœ¼ë¡œ ë‚˜ëˆ  F0 ì‹œí€€ìŠ¤ë¥¼ ì¶”ì •
            f0, _, _ = librosa.pyin(f, fmin=50, fmax=600, sr=sr, frame_length=2048, hop_length=512)
            f0_val = np.nanmedian(f0) # ëŒ€í‘œê°’: ì¤‘ì•™ê°’(ë…¸ì´ì¦ˆì— ê°•í•¨)
        except Exception:
            f0_val = np.nan
        if not np.isnan(f0_val):
            f0_val = float(np.clip(f0_val, 50, 600)) # ì•ˆì „ ë²”ìœ„ í´ë¦¬í•‘
        f0_all.append(f0_val)
    return np.array(f0_all, dtype=float)

# í”¼ì¹˜ ê²°ê³¼ ì¶œë ¥
def pitch_soft_probs_with_speakers(y: np.ndarray, sr: int,
                                   diar_df: Optional[pd.DataFrame] = None,
                                   win_s=WIN_S, hop_s=HOP_S,
                                   delta_st=PITCH_DELTA_ST, sigma=PITCH_SIGMA) -> PitchFrames:
    # í”„ë ˆì„ ë§Œë“¤ê¸° (ê°ì • ìª½ê³¼ ë™ì¼í•œ ìœˆë„/í™‰ ì‚¬ìš©)
    frames, centers, _ = frame_audio(y, sr, win_s, hop_s)
    if frames.size == 0:
        return PitchFrames(np.array([]), np.zeros((0,3)), np.array([]), {}, np.array([]), [])

    # 1) í”„ë ˆì„ë³„ F0
    f0_all = estimate_f0_per_frame(frames, sr)

    # 2) í”„ë ˆì„ë³„ speaker ë¼ë²¨ (ì—†ìœ¼ë©´ 'unknown')
    if diar_df is not None and not diar_df.empty:
        frame_speakers = assign_speaker_to_frames(centers, diar_df)
    else:
        frame_speakers = ["unknown"] * len(centers)

    # 3) í™”ìë³„ ê¸°ì¤€ F0 ì¤‘ì•™ê°’ ê³„ì‚° -> ê°™ì€ í™”ìì˜ í”„ë ˆì„ F0ë“¤ ì¤‘ NaNë¥¼ ì œì™¸í•˜ê³  median
    f0_med_per_speaker = {}
    for spk in set(frame_speakers):
        vals = f0_all[(np.array(frame_speakers) == spk) & (~np.isnan(f0_all))]
        if len(vals) > 0:
            f0_med_per_speaker[spk] = float(np.median(vals))
    # ê¸€ë¡œë²Œ ë°±ì—… -> í™”ìë³„ ê°’ì´ ì—†ì„ ë•Œ ì‚¬ìš©
    global_med = float(np.median(f0_all[~np.isnan(f0_all)])) if np.any(~np.isnan(f0_all)) else np.nan

    # 4) ì„¸ë¯¸í†¤ ë³€í™˜(í™”ìë³„ ê¸°ì¤€ì¹˜ë¥¼ ì‚¬ìš©) -> baseëŠ” í•´ë‹¹ í™”ìì˜ median F0
    st = np.zeros_like(centers, dtype=float)
    for i, (t, f0, spk) in enumerate(zip(centers, f0_all, frame_speakers)):
        base = f0_med_per_speaker.get(spk, global_med)
        if np.isnan(f0) or not np.isfinite(base):
            st[i] = 0.0  # ë¬´ì„±/ê¸°ì¤€ì¹˜ ì—†ìŒ â†’ Midë¡œ ìˆ˜ë ´
        else:
            st[i] = 12.0 * np.log2(max(f0, 1e-6) / base)

    # 5) ê°€ìš°ì‹œì•ˆ ì ìˆ˜ -> ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥ 
    # 3ê°œì˜ ì¤‘ì‹¬ì¹˜: [-Î”, 0, +Î”] semitone (Î”=delta_st)
    centers_st = np.array([-delta_st, 0.0, +delta_st])[:, None]  # (3,1)
    # ê° ì¤‘ì‹¬ì— ëŒ€í•´ ê°€ìš°ì‹œì•ˆ ì ìˆ˜
    scores = np.exp(-0.5 * ((st[None, :] - centers_st) / sigma) ** 2)  # (3, N)
    # í”„ë ˆì„ë§ˆë‹¤ 3ê°œ ì ìˆ˜ë¥¼ ì •ê·œí™”(ì†Œí”„íŠ¸ë§¥ìŠ¤) -> í™•ë¥  ë²¡í„°
    probs = (scores / (scores.sum(axis=0, keepdims=True) + 1e-9)).T      # (N,3)

    # ë¶ˆí™•ì‹¤ì„± ì¸¡ì •: ì—”íŠ¸ë¡œí”¼
    ent = -np.sum(probs * np.log(probs + 1e-9), axis=1)

    # ìµœì¢… íŒ¨í‚¤ì§•
    return PitchFrames(centers, probs, ent, f0_med_per_speaker, st, frame_speakers)

# ---------------------------
# ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„ (ê°€ì¤‘ í‰ê·  + ìµœëŒ€ê°’ + ì—”íŠ¸ë¡œí”¼)
# ---------------------------
# ë‹¨ì–´ë³„ ì‹œê°„ êµ¬ê°„(start, end)ì— í¬í•¨ëœëŠ ê°ì •, í”¼ì¹˜ í”„ë ˆì„ í™•ë¥ ì„ ëª¨ì•„ ìš”ì•½í•˜ê³  ì €ì¥
def aggregate_over_words(words_df: pd.DataFrame,
                         emo: EmotionFrames,
                         pitch: PitchFrames,
                         diar_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:
    """
    ë°˜í™˜ ì»¬ëŸ¼:
      start, end, word, speaker,
      emo_label, emo_entropy, emo_probs(json),
      pitch_label, pitch_entropy, pitch_probs(json)
    """
    out_rows = []
    # ê°ì • ë¼ë²¨ ì´ë¦„
    emo_labels = emo.labels if emo.labels else [f"class_{i}" for i in range(emo.probs.shape[1])]
    # í”¼ì¹˜ ë¼ë²¨ ì´ë¦„
    pitch_labels = ["low", "mid", "high"]

    # í”„ë ˆì„ â†’ ë¹ ë¥¸ ì¸ë±ì‹±ìš©
    etimes, eprobs, erms = emo.times, emo.probs, emo.rms
    ptimes, pprobs = pitch.times, pitch.probs

    for _, w in words_df.iterrows():
        # ë‹¨ì–´ ê²½ê³„ë¥¼ ì•½ê°„ í™•ì¥(WORD_PAD_Së§Œí¼ ì•ë’¤ íŒ¨ë”©) -> ê²½ê³„ ëˆ„ë½ ë°©ì§€
        t0 = float(w["start"]) - WORD_PAD_S
        t1 = float(w["end"]) + WORD_PAD_S

        # í™”ì ë¼ë²¨: ë‹¨ì–´ ì¤‘ì•™ìœ¼ë¡œ ê²°ì •(ë‹¤ì´ì–´ë¼ì´ì œì´ì…˜ì´ ìˆìœ¼ë©´)
        if diar_df is not None and not diar_df.empty:
            speaker = assign_speaker_at_time((t0+t1)/2.0, diar_df)
        else:
            speaker = "unknown"

        # ê°ì • ì§‘ê³„
        if etimes.size:
            # ë‹¨ì–´ êµ¬ê°„[t0, t1]ì— í¬í•¨ë˜ëŠ” ê°ì • í”„ë ˆì„ë§Œ ì„ íƒ
            mask_e = (etimes >= t0) & (etimes <= t1)
            Pe = eprobs[mask_e] # (num_frames_in_word, K)  K=ê°ì • í´ë˜ìŠ¤ ìˆ˜
            We = erms[mask_e]   # (num_frames_in_word,)    RMS: ì—ë„ˆì§€ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            if Pe.shape[0] > 0:
                # ê°€ì¤‘ì¹˜ í•©ì´ 0ì— ê°€ê¹ë‹¤ë©´ ê· ë“± ê°€ì¤‘ì¹˜ë¡œ ëŒ€ì²´
                if We.sum() <= 1e-12:
                    We = np.ones_like(We) / len(We) # ì •ê·œí™”(í•©=1)
                else:
                    We = We / We.sum()
                # ê°€ì¤‘ í‰ê· : ê° í”„ë ˆì„ í™•ë¥ ì— RMS ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ í•©ì‚° -> ë‹¨ì–´ë³„ ëŒ€í‘œ í™•ë¥  ë²¡í„°
                emo_mean = (Pe * We[:, None]).sum(axis=0)     # (K,)
                # í”„ë ˆì„ë³„ í™•ë¥  ì¤‘ ìµœëŒ€ê°’
                emo_max  = Pe.max(axis=0)
                # ì—”íŠ¸ë¡œí”¼: ë¶ˆí™•ì‹¤ì„± ì§€í‘œ(ë†’ì„ìˆ˜ë¡ ë¶„í¬ê°€ í¼ì ¸ìˆìŒ)
                emo_entropy = float(-(emo_mean * np.log(emo_mean + 1e-9)).sum())
                # ìµœìƒìœ„ ê°ì • ë¼ë²¨
                emo_top = emo_labels[int(np.argmax(emo_mean))]
                emo_probs_dict = {emo_labels[i]: float(emo_mean[i]) for i in range(len(emo_mean))}
            else:
                # í•´ë‹¹ êµ¬ê°„ì— ê°ì • í”„ë ˆì„ì´ ì—†ìœ¼ë©´ ê²°ì¸¡ ì²˜ë¦¬
                emo_entropy = float(np.nan); emo_top = ""
                emo_probs_dict = {}
        else:
            # ê°ì • í”„ë ˆì„ ìì²´ê°€ ì—†ì„ ë•Œ
            emo_entropy = float(np.nan); emo_top = ""
            emo_probs_dict = {}

        # í”¼ì¹˜ ì§‘ê³„
        if ptimes.size:
            # ë‹¨ì–´ êµ¬ê°„[t0, t1]ì— í¬í•¨ë˜ëŠ” í”¼ì¹˜ í”„ë ˆì„ë§Œ ì„ íƒ
            mask_p = (ptimes >= t0) & (ptimes <= t1)
            Pp = pprobs[mask_p]
            if Pp.shape[0] > 0:
                # í”¼ì¹˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê· ë“± ê°€ì¤‘ í‰ê· (ì›í•œë‹¤ë©´ RMS/voiced ê°€ì¤‘ì¹˜ë¡œ í™•ì¥ ê°€ëŠ¥)
                Wp = np.ones((Pp.shape[0],), dtype=float) / Pp.shape[0]
                # ë‹¨ì–´ êµ¬ê°„ì˜ ëŒ€í‘œ í”¼ì¹˜ í™•ë¥ 
                pitch_mean = (Pp * Wp[:, None]).sum(axis=0)
                # í”„ë ˆì„ë³„ ìµœëŒ€ê°’
                pitch_max  = Pp.max(axis=0)
                # ì—”íŠ¸ë¡œí”¼
                pitch_entropy = float(-(pitch_mean * np.log(pitch_mean + 1e-9)).sum())
                # ìµœìƒìœ„ í”¼ì¹˜ ë¼ë²¨
                pitch_top = pitch_labels[int(np.argmax(pitch_mean))]
                pitch_probs_dict = {pitch_labels[i]: float(pitch_mean[i]) for i in range(3)}
            else:
                pitch_entropy = float(np.nan); pitch_top = ""
                pitch_probs_dict = {}
        else:
            pitch_entropy = float(np.nan); pitch_top = ""
            pitch_probs_dict = {}

        out_rows.append({
            "start": float(w["start"]),
            "end": float(w["end"]),
            "word": str(w["word"]),
            "speaker": speaker,
            # ê°ì •
            "emo_label": emo_top,
            "emo_entropy": emo_entropy,
            "emo_probs": json.dumps(emo_probs_dict, ensure_ascii=False),
            # í”¼ì¹˜
            "pitch_label": pitch_top,
            "pitch_entropy": pitch_entropy,
            "pitch_probs": json.dumps(pitch_probs_dict, ensure_ascii=False),
        })

    return pd.DataFrame(out_rows)

# ---------------------------
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ---------------------------
def main():
    safe_mkdir(OUTPUT_DIR)

    # 1) ì…ë ¥ ë¡œë“œ & (ì„ íƒ) ë³´ì»¬ ë¶„ë¦¬
    src_wav = INPUT_AUDIO
    print(f"ğŸµ Input: {src_wav}")
    use_path = src_wav
    if USE_VOCAL_SEPARATION:
        # Demucsë¥¼ ì´ìš©í•´ ë³´ì»¬ë§Œ ì¶”ì¶œ
        use_path = separate_vocals_with_demucs(src_wav, OUTPUT_DIR)
    print(f"ğŸ¤ ë¶„ì„ ì˜¤ë””ì˜¤: {use_path}")

    # 2) ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
    if USE_WHISPER:
        try:
            # Whisperë¡œ ì˜¤ë””ì˜¤ì—ì„œ ë‹¨ì–´ ë‹¨ìœ„ (start, end, word) ì¶”ì¶œ
            words_df = whisper_word_timestamps(use_path, language=LANG)
        except Exception as e:
            print(f"âš ï¸ Whisper ì‹¤íŒ¨: {e}")
            if WORDS_CSV and os.path.exists(WORDS_CSV):
                words_df = pd.read_csv(WORDS_CSV)
                assert {"start","end","word"}.issubset(words_df.columns)
            else:
                raise
    else:
        # USE_WHISPER=False â†’ ë°˜ë“œì‹œ WORDS_CSV ì œê³µí•´ì•¼ í•¨
        if WORDS_CSV and os.path.exists(WORDS_CSV):
            words_df = pd.read_csv(WORDS_CSV)
            assert {"start","end","word"}.issubset(words_df.columns)
        else:
            raise RuntimeError("USE_WHISPER=False ì¸ ê²½ìš° WORDS_CSV ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # Whisper ê²°ê³¼ CSV ì €ì¥ ë° ë¯¸ë¦¬ë³´ê¸°
    words_raw_csv = os.path.join(OUTPUT_DIR, "words_raw.csv")
    write_csv(words_df, words_raw_csv)  # â‘  Whisper ì›ë³¸ ë‹¨ì–´ CSV ì €ì¥
    preview(words_df, "Whisper ë‹¨ì–´")  # â‘¡ ì½˜ì†” ë¯¸ë¦¬ë³´ê¸°
    print(f"ğŸ“„ ë‹¨ì–´ íŒŒì¼ ì €ì¥: {words_raw_csv}")  # â‘¢ ì €ì¥ ê²½ë¡œ ë¡œê·¸

    # 3) í™”ì ë¶„ë¦¬(ì„ íƒ)
    diar_df = pd.DataFrame(columns=["start","end","speaker"])
    if USE_DIARIZATION:
        try:
            # pyannote.audio ëª¨ë¸ ì‹¤í–‰ â†’ í™”ì êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
            segs = run_diarization(use_path, DIARIZATION_MODEL_ID)
            diar_df = diar_to_dataframe(segs)
            diar_csv = os.path.join(OUTPUT_DIR, "diarization_segments.csv")
            write_csv(diar_df, diar_csv)
            # ë‹¨ì–´ì— speaker íƒœê¹…(ì¤‘ì•™ì‹œê° ê¸°ì¤€)
            words_df = annotate_words_with_speaker(words_df, diar_df)
        except Exception as e:
            print(f"âš ï¸ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")
            diar_df = pd.DataFrame(columns=["start","end","speaker"])
            words_df["speaker"] = "unknown"
    else:
        # í™”ì ë¶„ë¦¬ ê¸°ëŠ¥ ë¹„í™œì„±í™” â†’ speaker="unknown"
        words_df["speaker"] = "unknown"

    # ë‹¨ì–´ + í™”ì CSV ì €ì¥
    words_df = annotate_words_with_speaker(words_df, diar_df)
    words_spk_csv = os.path.join(OUTPUT_DIR, "words_with_speaker.csv")
    write_csv(words_df, words_spk_csv)
    preview(words_df, "ë‹¨ì–´+í™”ì ë¼ë²¨")
    print(f"ğŸ“„ ë‹¨ì–´+í™”ì íŒŒì¼ ì €ì¥: {words_spk_csv}")

    # 4) ì˜¤ë””ì˜¤ ë¡œë“œ (ëª¨ë…¸/16k)
    y, sr = load_audio(use_path)
    y, sr = ensure_mono_16k(y, sr)

    # 5) ê°ì •: í”„ë ˆì„ ì¸í¼ëŸ°ìŠ¤
    print("ğŸ§  Emotion frame inference...")
    emo = emotion_frame_probs(y, sr, EMO_MODEL_ID)

    # 6) í”¼ì¹˜: í™”ìë³„ ê¸°ì¤€ì¹˜ë¡œ ì†Œí”„íŠ¸ í™•ë¥ 
    print("ğŸ¼ Pitch soft-prob (speaker-aware)...")
    pitch = pitch_soft_probs_with_speakers(y, sr, diar_df, WIN_S, HOP_S, PITCH_DELTA_ST, PITCH_SIGMA)

    # 7) ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„
    print("ğŸ§® Aggregating per word...")
    out_df = aggregate_over_words(words_df, emo, pitch, diar_df)

    # 8) ì €ì¥
    out_csv = os.path.join(OUTPUT_DIR, "words_emotion_pitch.csv")
    write_csv(out_df, out_csv)
    print("âœ… Done.")


if __name__ == "__main__":
    main()
