# filename: full_emotion_pitch_with_diarization.py
# --------------------------------------------------
# ì…ë ¥ ì˜¤ë””ì˜¤ -> (ì„ íƒ)ë³´ì»¬ ë¶„ë¦¬ -> Whisper ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
#           -> (ì„ íƒ)í™”ì ë¶„ë¦¬(pyannote) -> í”„ë ˆì„ ê°ì • í™•ë¥ (W2V2 SER)
#           -> í”¼ì¹˜(F0->ì„¸ë¯¸í†¤->ì†Œí”„íŠ¸ í™•ë¥ , í™”ìë³„ ê¸°ì¤€ì¹˜) -> ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„
#           -> CSV ì €ì¥(start, end, word, speaker, emo_*, pitch_*)
# --------------------------------------------------
import sys

# ===== 1) ì—¬ê¸°ë§Œ ì±„ìš°ë©´ ë°”ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤! =====
INPUT_AUDIO = r"C:\Users\user\PycharmProjects\emotion_subtitle_improve\sample.wav"   # â† ì…ë ¥ ì˜¤ë””ì˜¤ ê²½ë¡œ
OUTPUT_DIR  = r"C:\Users\user\PycharmProjects\emotion_subtitle_improve\sample_test_2"      # â† ì¶œë ¥ í´ë”
LANG        = "en"                           # Whisper ê°•ì œ ì–¸ì–´ (ì˜ˆ: "en", "ko")
WORDS_CSV   = None                           # Whisper ëŒ€ì‹  ì“¸ ë‹¨ì–´ CSV(start,end,word). ì—†ìœ¼ë©´ None

USE_VOCAL_SEPARATION = True                  # Spleeter/Demucsê°€ ìˆìœ¼ë©´ ë³´ì»¬ ë¶„ë¦¬ ì‚¬ìš©
USE_WHISPER          = True                  # openai-whisper ì„¤ì¹˜ ì‹œ ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ìë™ ì¶”ì¶œ
USE_DIARIZATION      = True                  # pyannote.audio ì„¤ì¹˜ + HF_TOKEN í•„ìš”

# ëª¨ë¸ ID
EMO_MODEL_ID          = "superb/hubert-large-superb-er"         # 7-class SER
DIARIZATION_MODEL_ID  = "pyannote/speaker-diarization-3.1"            # pyannote diarization pipeline

# í”„ë ˆì´ë° íŒŒë¼ë¯¸í„° (ê°ì •/í”¼ì¹˜ ê³µí†µ ê·¸ë¦¬ë“œ)
WIN_S = 0.5        # ìœˆë„ ê¸¸ì´(ì´ˆ)
HOP_S = 0.25       # í™‰(ì´ˆ)
WORD_PAD_S = 0.05  # ë‹¨ì–´ ê²½ê³„ íŒ¨ë”©(Â±ì´ˆ)

# í”¼ì¹˜ ì†Œí”„íŠ¸ ë¶„ë¥˜(ì„¸ë¯¸í†¤) íŒŒë¼ë¯¸í„°
PITCH_DELTA_ST = 2.0   # class centers [-Î”, 0, +Î”] st
PITCH_SIGMA    = 1.2   # Gaussian sigma (st)

HF_TOKEN = "HF_TOKEN_REDACTED"  # â† ì—¬ê¸°ì— ë³¸ì¸ í† í°

# ==================================================

import os, json, math
from dataclasses import dataclass
from typing import Optional, Tuple, List
import numpy as np
import pandas as pd
import soundfile as sf
import librosa
import torch
import torch.nn.functional as F
import subprocess, shutil
from pathlib import Path

from transformers import AutoModelForAudioClassification
from transformers import AutoFeatureExtractor as AudioProcessorClass

from pydub import AudioSegment

AudioSegment.converter = r"C:\ffmpeg-7.1.1-essentials_build\bin\ffmpeg.exe"

def preview(df: pd.DataFrame, name: str, n: int = 10):
    try:
        print(f"ğŸ“ {name} ë¯¸ë¦¬ë³´ê¸° (top {n})")
        print(df.head(n).to_string(index=False))
    except Exception as e:
        print(f"(ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e})")

# ---------------------------
# ìœ í‹¸
# ---------------------------
def safe_mkdir(p: str):
    os.makedirs(p, exist_ok=True)

def ensure_mono_16k(y, sr, target_sr=16000):
    if y.ndim == 2:
        y = librosa.to_mono(y.T)
    if sr != target_sr:
        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)
        sr = target_sr
    return y, sr

def write_csv(df: pd.DataFrame, path: str):
    df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ Saved: {path}")

def load_audio(path: str):
    y, sr = librosa.load(path, sr=None, mono=True)
    return y, sr

def rms_per_frame(frames: np.ndarray) -> np.ndarray:
    return np.sqrt((frames**2).mean(axis=1)) if len(frames) else np.array([])

# ---------------------------
# ë³´ì»¬ ë¶„ë¦¬
# ---------------------------
def separate_vocals_with_demucs(wav_path: str, session_dir: str) -> str:
    """
    ì…ë ¥ WAVì—ì„œ ë³´ì»¬ë§Œ Demucsë¡œ ë¶„ë¦¬í•´ session_dir/separation/vocals.wavë¡œ ì €ì¥.
    ì‹¤íŒ¨í•˜ë©´ ì˜ˆì™¸ ëŒ€ì‹  ì›ë³¸ ê²½ë¡œë¥¼ ë°˜í™˜(í´ë°±)í•˜ë„ë¡ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬.
    """
    from pathlib import Path
    import subprocess, shutil, sys
    import torch

    wav_path    = str(Path(wav_path).resolve())
    session_dir = Path(session_dir).resolve()
    out_root    = session_dir / "separation" / "demucs_out"
    out_root.mkdir(parents=True, exist_ok=True)

    # GPU ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ ì¥ì¹˜ ìë™ ì„ íƒ
    device = "cuda" if torch.cuda.is_available() else "cpu"

    cmd = [
        sys.executable, "-m", "demucs.separate",
        "-n", "htdemucs",
        "--two-stems", "vocals",
        "-d", device,                # auto(cu/cpu)
        "-o", str(out_root),
        wav_path
    ]

    proc = subprocess.run(
        cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore"
    )

    if proc.returncode != 0:
        print("âŒ Demucs ì‹¤íŒ¨")
        print("â”€â”€ stdout â”€â”€")
        print(proc.stdout.strip())
        print("â”€â”€ stderr â”€â”€")
        print(proc.stderr.strip())
        print("âš ï¸ ë³´ì»¬ ë¶„ë¦¬ ì—†ì´ ì›ë³¸ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return wav_path  # í´ë°±

    # ë²„ì „/í”Œë«í¼ë³„ í´ë” ì°¨ì´ë¥¼ í—ˆìš©: ì–´ë””ë“  vocals.wavë§Œ ì°¾ì•„ì˜´
    candidates = list(out_root.rglob("vocals.wav"))
    if not candidates:
        print("âŒ Demucs ì¶œë ¥ íŒŒì¼(vocals.wav)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("â”€â”€ stdout â”€â”€")
        print(proc.stdout.strip())
        print("â”€â”€ stderr â”€â”€")
        print(proc.stderr.strip())
        print("âš ï¸ ë³´ì»¬ ë¶„ë¦¬ ì—†ì´ ì›ë³¸ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return wav_path  # í´ë°±

    demucs_vocals = candidates[0]
    fixed = session_dir / "separation" / "vocals.wav"
    fixed.parent.mkdir(parents=True, exist_ok=True)
    shutil.copyfile(demucs_vocals, fixed)
    return str(fixed)

# ---------------------------
# Whisper ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
# ---------------------------
def whisper_word_timestamps(audio_path: str, language: Optional[str] = "en") -> pd.DataFrame:
    import whisper  # pip install openai-whisper
    print("ğŸ”¤ Whisper(small) ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ...")
    model = whisper.load_model("small")
    result = model.transcribe(audio_path, language=language, word_timestamps=True, verbose=False)
    words = []
    for seg in result.get("segments", []):
        for w in seg.get("words", []):
            words.append({"start": w["start"], "end": w["end"], "word": w["word"].strip()})
    df = pd.DataFrame(words)
    if df.empty:
        raise RuntimeError("Whisperê°€ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return df

# ---------------------------
# í™”ì ë¶„ë¦¬(ë‹¤ì´ì–´ë¼ì´ì œì´ì…˜)
# ---------------------------
@dataclass
class DiarizationSeg:
    start: float
    end: float
    speaker: str

def run_diarization(audio_path: str, model_id=DIARIZATION_MODEL_ID) -> List[DiarizationSeg]:
    """
    pyannote.audio Pipeline ì‚¬ìš©. ì „ì—­ ìƒìˆ˜ HF_TOKEN ì‚¬ìš©.
    ë°˜í™˜: [DiarizationSeg(...)] ë¦¬ìŠ¤íŠ¸
    """
    print("ğŸ‘¥ í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘ (pyannote)...")
    from pyannote.audio import Pipeline

    if not HF_TOKEN or not isinstance(HF_TOKEN, str):
        raise RuntimeError("HF_TOKENì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ HuggingFace í† í°ì„ ë„£ì–´ì£¼ì„¸ìš”.")

    # í™˜ê²½ë³€ìˆ˜ ëŒ€ì‹  í•˜ë“œì½”ë”©ëœ í† í° ì‚¬ìš©
    pipeline = Pipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN)

    diar = pipeline(audio_path)  # Annotation
    segs: List[DiarizationSeg] = []
    for turn, _, speaker in diar.itertracks(yield_label=True):
        segs.append(DiarizationSeg(start=float(turn.start), end=float(turn.end), speaker=str(speaker)))
    segs.sort(key=lambda s: s.start)
    print(f"ğŸ‘¥ í™”ì ì„¸ê·¸ë¨¼íŠ¸ {len(segs)}ê°œ")
    return segs

def diar_to_dataframe(segs: List[DiarizationSeg]) -> pd.DataFrame:
    return pd.DataFrame([{"start": s.start, "end": s.end, "speaker": s.speaker} for s in segs])

def assign_speaker_at_time(t: float, diar_df: pd.DataFrame, default="unknown") -> str:
    # tê°€ í¬í•¨ë˜ëŠ” ì²« ì„¸ê·¸ë¨¼íŠ¸ì˜ speaker ë°˜í™˜ (ê²¹ì¹˜ë©´ ê°€ì¥ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ ìš°ì„ )
    hits = diar_df[(diar_df["start"] <= t) & (diar_df["end"] >= t)]
    if hits.empty:
        return default
    # ê°€ì¥ ê¸´ ì„¸ê·¸ë¨¼íŠ¸
    lens = (hits["end"] - hits["start"]).to_numpy()
    return hits.iloc[int(lens.argmax())]["speaker"]

def assign_speaker_to_frames(frame_times: np.ndarray, diar_df: pd.DataFrame) -> List[str]:
    return [assign_speaker_at_time(float(t), diar_df) for t in frame_times]

def annotate_words_with_speaker(words_df: pd.DataFrame, diar_df: pd.DataFrame) -> pd.DataFrame:
    """
    ê° ë‹¨ì–´ì— ìŠ¤í”¼ì»¤ ë¼ë²¨ ë¶€ì—¬: ë‹¨ì–´ ì¤‘ì•™ ì‹œê° ê¸°ì¤€ìœ¼ë¡œ ë§¤í•‘(ê°„ë‹¨/ê²¬ê³ )
    """
    speakers = []
    for _, w in words_df.iterrows():
        mid = (float(w["start"]) + float(w["end"])) / 2.0
        speakers.append(assign_speaker_at_time(mid, diar_df))
    out = words_df.copy()
    out["speaker"] = speakers
    return out

# ---------------------------
# ê°ì •: í”„ë ˆì„ ì¸í¼ëŸ°ìŠ¤
# ---------------------------
@dataclass
class EmotionFrames:
    times: np.ndarray
    probs: np.ndarray
    rms: np.ndarray
    labels: List[str]

def frame_audio(y: np.ndarray, sr: int, win_s=WIN_S, hop_s=HOP_S):
    win = int(win_s * sr); hop = int(hop_s * sr)
    frames, centers = [], []
    for start in range(0, max(1, len(y)-win+1), hop):
        end = start + win
        if end > len(y): break
        frames.append(y[start:end])
        centers.append((start + end) / 2 / sr)
    frames = np.stack(frames) if frames else np.empty((0,))
    centers = np.array(centers, dtype=float)
    rms = rms_per_frame(frames)
    return frames, centers, rms

def emotion_frame_probs(y: np.ndarray, sr: int, model_id=EMO_MODEL_ID) -> EmotionFrames:
    # Processor (AutoProcessor or AutoFeatureExtractor)
    proc = AudioProcessorClass.from_pretrained(model_id)
    model = AutoModelForAudioClassification.from_pretrained(model_id)
    model.eval()

    y, sr = ensure_mono_16k(y, sr, target_sr=getattr(proc, "sampling_rate", 16000))
    frames, centers, rms = frame_audio(y, sr, WIN_S, HOP_S)
    if frames.size == 0:
        return EmotionFrames(np.array([]), np.zeros((0, model.config.num_labels)), np.array([]), [])

    inputs = proc(frames.tolist(), sampling_rate=sr, return_tensors="pt", padding=True)
    with torch.no_grad():
        logits = model(**{k: v for k, v in inputs.items() if k in ["input_values", "attention_mask"]}).logits
        probs = F.softmax(logits, dim=-1).cpu().numpy()
    labels = [model.config.id2label[i] for i in range(model.config.num_labels)]
    return EmotionFrames(centers, probs, rms, labels)


# ---------------------------
# í”¼ì¹˜: F0 -> ì„¸ë¯¸í†¤ -> ì†Œí”„íŠ¸ í™•ë¥  (í™”ìë³„ ê¸°ì¤€ì¹˜)
# ---------------------------
@dataclass
class PitchFrames:
    times: np.ndarray
    probs: np.ndarray      # (N,3) [low, mid, high]
    entropy: np.ndarray
    f0_med_per_speaker: dict
    st_values: np.ndarray
    frame_speakers: List[str]

def estimate_f0_per_frame(frames: np.ndarray, sr: int) -> np.ndarray:
    """ê° í”„ë ˆì„ì—ì„œ pyinìœ¼ë¡œ F0 ëŒ€í‘œê°’(ì¤‘ì•™ê°’) ì¶”ì •"""
    f0_all = []
    for f in frames:
        try:
            f0, _, _ = librosa.pyin(f, fmin=50, fmax=600, sr=sr, frame_length=2048, hop_length=512)
            f0_val = np.nanmedian(f0)
        except Exception:
            f0_val = np.nan
        if not np.isnan(f0_val):
            f0_val = float(np.clip(f0_val, 50, 600))
        f0_all.append(f0_val)
    return np.array(f0_all, dtype=float)

def pitch_soft_probs_with_speakers(y: np.ndarray, sr: int,
                                   diar_df: Optional[pd.DataFrame] = None,
                                   win_s=WIN_S, hop_s=HOP_S,
                                   delta_st=PITCH_DELTA_ST, sigma=PITCH_SIGMA) -> PitchFrames:
    frames, centers, _ = frame_audio(y, sr, win_s, hop_s)
    if frames.size == 0:
        return PitchFrames(np.array([]), np.zeros((0,3)), np.array([]), {}, np.array([]), [])

    # 1) í”„ë ˆì„ë³„ F0
    f0_all = estimate_f0_per_frame(frames, sr)

    # 2) í”„ë ˆì„ë³„ speaker ë¼ë²¨ (ì—†ìœ¼ë©´ 'unknown')
    if diar_df is not None and not diar_df.empty:
        frame_speakers = assign_speaker_to_frames(centers, diar_df)
    else:
        frame_speakers = ["unknown"] * len(centers)

    # 3) í™”ìë³„ F0 ì¤‘ì•™ê°’
    f0_med_per_speaker = {}
    for spk in set(frame_speakers):
        vals = f0_all[(np.array(frame_speakers) == spk) & (~np.isnan(f0_all))]
        if len(vals) > 0:
            f0_med_per_speaker[spk] = float(np.median(vals))
    # ê¸€ë¡œë²Œ ë°±ì—…
    global_med = float(np.median(f0_all[~np.isnan(f0_all)])) if np.any(~np.isnan(f0_all)) else np.nan

    # 4) ì„¸ë¯¸í†¤ ë³€í™˜(í™”ìë³„ ê¸°ì¤€ì¹˜ë¥¼ ì‚¬ìš©)
    st = np.zeros_like(centers, dtype=float)
    for i, (t, f0, spk) in enumerate(zip(centers, f0_all, frame_speakers)):
        base = f0_med_per_speaker.get(spk, global_med)
        if np.isnan(f0) or not np.isfinite(base):
            st[i] = 0.0  # ë¬´ì„±/ê¸°ì¤€ì¹˜ ì—†ìŒ â†’ Midë¡œ ìˆ˜ë ´
        else:
            st[i] = 12.0 * np.log2(max(f0, 1e-6) / base)

    # 5) ê°€ìš°ì‹œì•ˆ ì ìˆ˜ -> ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥ 
    centers_st = np.array([-delta_st, 0.0, +delta_st])[:, None]  # (3,1)
    scores = np.exp(-0.5 * ((st[None, :] - centers_st) / sigma) ** 2)  # (3, N)
    probs = (scores / (scores.sum(axis=0, keepdims=True) + 1e-9)).T      # (N,3)

    ent = -np.sum(probs * np.log(probs + 1e-9), axis=1)

    return PitchFrames(centers, probs, ent, f0_med_per_speaker, st, frame_speakers)


# ---------------------------
# ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„ (ê°€ì¤‘ í‰ê·  + ìµœëŒ€ê°’ + ì—”íŠ¸ë¡œí”¼)
# ---------------------------
def aggregate_over_words(words_df: pd.DataFrame,
                         emo: EmotionFrames,
                         pitch: PitchFrames,
                         diar_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:
    """
    ë°˜í™˜ ì»¬ëŸ¼:
      start, end, word, speaker,
      emo_label, emo_entropy, emo_probs(json),
      pitch_label, pitch_entropy, pitch_probs(json)
    """
    out_rows = []
    emo_labels = emo.labels if emo.labels else [f"class_{i}" for i in range(emo.probs.shape[1])]
    pitch_labels = ["low", "mid", "high"]

    # í”„ë ˆì„ â†’ ë¹ ë¥¸ ì¸ë±ì‹±ìš©
    etimes, eprobs, erms = emo.times, emo.probs, emo.rms
    ptimes, pprobs = pitch.times, pitch.probs

    for _, w in words_df.iterrows():
        t0 = float(w["start"]) - WORD_PAD_S
        t1 = float(w["end"]) + WORD_PAD_S

        # speaker: ë‹¨ì–´ ì¤‘ì•™ìœ¼ë¡œ ê²°ì •(ë‹¤ì´ì–´ë¼ì´ì œì´ì…˜ì´ ìˆìœ¼ë©´)
        if diar_df is not None and not diar_df.empty:
            speaker = assign_speaker_at_time((t0+t1)/2.0, diar_df)
        else:
            speaker = "unknown"

        # ê°ì • ì§‘ê³„
        if etimes.size:
            mask_e = (etimes >= t0) & (etimes <= t1)
            Pe = eprobs[mask_e]
            We = erms[mask_e]
            if Pe.shape[0] > 0:
                if We.sum() <= 1e-12:
                    We = np.ones_like(We) / len(We)
                else:
                    We = We / We.sum()
                emo_mean = (Pe * We[:, None]).sum(axis=0)     # (K,)
                emo_max  = Pe.max(axis=0)
                emo_entropy = float(-(emo_mean * np.log(emo_mean + 1e-9)).sum())
                emo_top = emo_labels[int(np.argmax(emo_mean))]
                emo_probs_dict = {emo_labels[i]: float(emo_mean[i]) for i in range(len(emo_mean))}
            else:
                emo_entropy = float(np.nan); emo_top = ""
                emo_probs_dict = {}
        else:
            emo_entropy = float(np.nan); emo_top = ""
            emo_probs_dict = {}

        # í”¼ì¹˜ ì§‘ê³„
        if ptimes.size:
            mask_p = (ptimes >= t0) & (ptimes <= t1)
            Pp = pprobs[mask_p]
            if Pp.shape[0] > 0:
                Wp = np.ones((Pp.shape[0],), dtype=float) / Pp.shape[0]
                pitch_mean = (Pp * Wp[:, None]).sum(axis=0)
                pitch_max  = Pp.max(axis=0)
                pitch_entropy = float(-(pitch_mean * np.log(pitch_mean + 1e-9)).sum())
                pitch_top = pitch_labels[int(np.argmax(pitch_mean))]
                pitch_probs_dict = {pitch_labels[i]: float(pitch_mean[i]) for i in range(3)}
            else:
                pitch_entropy = float(np.nan); pitch_top = ""
                pitch_probs_dict = {}
        else:
            pitch_entropy = float(np.nan); pitch_top = ""
            pitch_probs_dict = {}

        out_rows.append({
            "start": float(w["start"]),
            "end": float(w["end"]),
            "word": str(w["word"]),
            "speaker": speaker,
            # ê°ì •
            "emo_label": emo_top,
            "emo_entropy": emo_entropy,
            "emo_probs": json.dumps(emo_probs_dict, ensure_ascii=False),
            # í”¼ì¹˜
            "pitch_label": pitch_top,
            "pitch_entropy": pitch_entropy,
            "pitch_probs": json.dumps(pitch_probs_dict, ensure_ascii=False),
        })

    return pd.DataFrame(out_rows)


# ---------------------------
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ---------------------------
def main():
    safe_mkdir(OUTPUT_DIR)

    # 1) ì…ë ¥ ë¡œë“œ & (ì„ íƒ) ë³´ì»¬ ë¶„ë¦¬
    src_wav = INPUT_AUDIO
    print(f"ğŸµ Input: {src_wav}")
    use_path = src_wav
    if USE_VOCAL_SEPARATION:
        use_path = separate_vocals_with_demucs(src_wav, OUTPUT_DIR)
    print(f"ğŸ¤ ë¶„ì„ ì˜¤ë””ì˜¤: {use_path}")

    # 2) ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
    if USE_WHISPER:
        try:
            words_df = whisper_word_timestamps(use_path, language=LANG)
        except Exception as e:
            print(f"âš ï¸ Whisper ì‹¤íŒ¨: {e}")
            if WORDS_CSV and os.path.exists(WORDS_CSV):
                words_df = pd.read_csv(WORDS_CSV)
                assert {"start","end","word"}.issubset(words_df.columns)
            else:
                raise
    else:
        if WORDS_CSV and os.path.exists(WORDS_CSV):
            words_df = pd.read_csv(WORDS_CSV)
            assert {"start","end","word"}.issubset(words_df.columns)
        else:
            raise RuntimeError("USE_WHISPER=False ì¸ ê²½ìš° WORDS_CSV ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    words_raw_csv = os.path.join(OUTPUT_DIR, "words_raw.csv")
    write_csv(words_df, words_raw_csv)  # â‘  Whisper ì›ë³¸ ë‹¨ì–´ CSV ì €ì¥
    preview(words_df, "Whisper ë‹¨ì–´")  # â‘¡ ì½˜ì†” ë¯¸ë¦¬ë³´ê¸°
    print(f"ğŸ“„ ë‹¨ì–´ íŒŒì¼ ì €ì¥: {words_raw_csv}")  # â‘¢ ì €ì¥ ê²½ë¡œ ë¡œê·¸

    # 3) í™”ì ë¶„ë¦¬(ì„ íƒ)
    diar_df = pd.DataFrame(columns=["start","end","speaker"])
    if USE_DIARIZATION:
        try:
            segs = run_diarization(use_path, DIARIZATION_MODEL_ID)
            diar_df = diar_to_dataframe(segs)
            diar_csv = os.path.join(OUTPUT_DIR, "diarization_segments.csv")
            write_csv(diar_df, diar_csv)
            # ë‹¨ì–´ì— speaker íƒœê¹…(ì¤‘ì•™ì‹œê° ê¸°ì¤€)
            words_df = annotate_words_with_speaker(words_df, diar_df)
        except Exception as e:
            print(f"âš ï¸ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")
            diar_df = pd.DataFrame(columns=["start","end","speaker"])
            words_df["speaker"] = "unknown"
    else:
        words_df["speaker"] = "unknown"

    words_df = annotate_words_with_speaker(words_df, diar_df)
    words_spk_csv = os.path.join(OUTPUT_DIR, "words_with_speaker.csv")
    write_csv(words_df, words_spk_csv)
    preview(words_df, "ë‹¨ì–´+í™”ì ë¼ë²¨")
    print(f"ğŸ“„ ë‹¨ì–´+í™”ì íŒŒì¼ ì €ì¥: {words_spk_csv}")

    # 4) ì˜¤ë””ì˜¤ ë¡œë“œ (ëª¨ë…¸/16k)
    y, sr = load_audio(use_path)
    y, sr = ensure_mono_16k(y, sr)

    # 5) ê°ì •: í”„ë ˆì„ ì¸í¼ëŸ°ìŠ¤
    print("ğŸ§  Emotion frame inference...")
    emo = emotion_frame_probs(y, sr, EMO_MODEL_ID)

    # 6) í”¼ì¹˜: í™”ìë³„ ê¸°ì¤€ì¹˜ë¡œ ì†Œí”„íŠ¸ í™•ë¥ 
    print("ğŸ¼ Pitch soft-prob (speaker-aware)...")
    pitch = pitch_soft_probs_with_speakers(y, sr, diar_df, WIN_S, HOP_S, PITCH_DELTA_ST, PITCH_SIGMA)

    # 7) ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„
    print("ğŸ§® Aggregating per word...")
    out_df = aggregate_over_words(words_df, emo, pitch, diar_df)

    # 8) ì €ì¥
    out_csv = os.path.join(OUTPUT_DIR, "words_emotion_pitch.csv")
    write_csv(out_df, out_csv)
    print("âœ… Done.")


if __name__ == "__main__":
    main()
