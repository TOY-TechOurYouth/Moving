import os, sys, json, shutil, subprocess
from dataclasses import dataclass
from typing import Optional, List, Tuple
from pathlib import Path

import numpy as np
import pandas as pd
import librosa
import torch
import torch.nn.functional as F

from transformers import AutoModelForAudioClassification, AutoFeatureExtractor

# ===================== ì‚¬ìš©ì ì„¤ì • =====================
INPUT_AUDIO = r"C:\Users\user\PycharmProjects\emotion_subtitle_improve\sample.wav"
OUTPUT_DIR  = r"C:\Users\user\PycharmProjects\emotion_subtitle_improve\sample_test_1-2"
LANG        = "en"        # whisper ë‹¨ì–´ ì¶”ì¶œ ê°•ì œ ì–¸ì–´ ("ko" ê°€ëŠ¥)
WORDS_CSV   = None        # ë¯¸ì‚¬ìš© ì‹œ None, ì‚¬ìš© ì‹œ (start,end,word) words.raw.csv ìƒì„±

USE_WHISPER          = True # Whisper ì‚¬ìš© ì—¬ë¶€
USE_DIARIZATION      = True # í™”ì ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€(pynnote)
USE_VOCAL_SEPARATION = True # ë³´ì»¬ ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€(Demusc)

# í™”ì ë¼ë²¨ë§ ë°©ì‹ í† ê¸€
# True  -> ì‹œê°„ íë¦„ì—ì„œ í™”ìê°€ ë°”ë€” ë•Œë§ˆë‹¤ 0,1,2,3... (ë™ì¼ í™”ì ì¬ë“±ì¥ë„ ìƒˆ ë²ˆí˜¸)
# False -> pyannoteì˜ í´ëŸ¬ìŠ¤í„° ë¼ë²¨(SPEAKER_00 ë“±)ì„ ë³´ì¡´ -> ì²˜ìŒ ë°©ì‹
SEQUENTIAL_SPEAKER_LABELS = True

# ì •í™•ë„ 91% ê°ì • ëª¨ë¸
EMO_MODEL_ID         = "firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3"
# í™”ì ë¶„ë¦¬ ëª¨ë¸
DIARIZATION_MODEL_ID = "pyannote/speaker-diarization-3.1"

# í”„ë ˆì´ë°
WIN_S = 0.5     # í”„ë ˆì„ ìœˆë„ìš° í¬ê¸°(ì´ˆ ë‹¨ìœ„)
HOP_S = 0.25    # í”„ë ˆì„ ì´ë™ ê°„ê²©(ì´ˆ ë‹¨ìœ„)

# í”¼ì¹˜ ì†Œí”„íŠ¸ ë¶„ë¥˜(ì„¸ë¯¸í†¤)
PITCH_DELTA_ST = 2.0    # ê¸°ì¤€ ìŒì •ì—ì„œ ëª‡ ì„¸ë¯¸í†¤ ë–¨ì–´ì¡ŒëŠ”ì§€ë¥¼ low/mid/highë¡œ êµ¬ë¶„í• ì§€ ê¸°ì¤€
PITCH_SIGMA    = 1.2    # í™•ë¥  ë¶„í¬ ê³„ì‚° ì‹œ ê°€ìš°ì‹œì•ˆ ë¶„ì‚° ê°’

# pyannote diarization ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ HuggingFace API í† í°
HF_TOKEN = "HF_TOKEN_REDACTED"
# ======================================================

# ---------------------------
# ìœ í‹¸ / IO
# ---------------------------
# ì§€ì •í•œ ê²½ë¡œ pì— ë””ë ‰í† ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ìƒì„±
def safe_mkdir(p: str):
    os.makedirs(p, exist_ok=True)

# CSV íŒŒì¼ ì €ì¥ í•¨ìˆ˜
def write_csv(df: pd.DataFrame, path: str):
    df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ Saved: {path}")

# librosaë¥¼ ì´ìš©í•´ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
def load_audio(path: str):
    y, sr = librosa.load(path, sr=None, mono=True)
    return y, sr

# ì˜¤ë””ì˜¤ ì‹ í˜¸ë¥¼ ë‹¨ì¼ ì±„ë„(mono) & ì§€ì •ëœ ìƒ˜í”Œë§ë ˆì´íŠ¸ë¡œ ë³€í™˜
def ensure_mono_sr(y, sr, target_sr=16000):
    if y.ndim == 2:
        y = librosa.to_mono(y.T)
    if sr != target_sr:
        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)
        sr = target_sr
    return y, sr

# ì…ë ¥ëœ ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ ê°ê°ì— ëŒ€í•´ RMS(root mean square) ê³„ì‚° -> ë‹¨ì–´ ì§‘ê³„ êµ¬ê°„ì—ì„œ ì“°ì„
def rms_per_frame(frames: np.ndarray) -> np.ndarray:
    return np.sqrt((frames**2).mean(axis=1)) if len(frames) else np.array([])

# ì½˜ì†”ì— ê°„ë‹¨íˆ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥ í•¨ìˆ˜
def preview(df: pd.DataFrame, name: str, n: int = 10):
    try:
        print(f"ğŸ“ {name} ë¯¸ë¦¬ë³´ê¸° (top {n})")
        print(df.head(n).to_string(index=False))
    except Exception as e:
        print(f"(ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e})")

# ---------------------------
# ë³´ì»¬ ë¶„ë¦¬
# ---------------------------
def separate_vocals_with_demucs(wav_path: str, session_dir: str) -> str:
    # ì…ë ¥ ì˜¤ë””ì˜¤ ê²½ë¡œì™€ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë°˜í™˜
    wav_path    = str(Path(wav_path).resolve())
    session_dir = Path(session_dir).resolve()
    # ë¶„ë¦¬ ê²°ê³¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
    out_root    = session_dir / "separation" / "demucs_out"
    out_root.mkdir(parents=True, exist_ok=True)

    # GPUê°€ ìˆìœ¼ë©´ cuda, ì—†ìœ¼ë©´ cpu ì‚¬ìš©
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # Demucs ì‹¤í–‰ ëª…ë ¹ì–´ êµ¬ì„±
    cmd = [
        sys.executable, "-m", "demucs.separate",
        "-n", "htdemucs",
        "--two-stems", "vocals",
        "-d", device,
        "-o", str(out_root),
        wav_path
    ]
    # subprocessë¡œ ì™¸ë¶€ ëª…ë ¹ì–´ ì‹¤í–‰
    proc = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore")
    # ë§Œì•½ demucs ì‹¤í–‰ ì‹¤íŒ¨ -> ì›ë³¸ ì˜¤ë””ì˜¤ ê²½ë¡œ ë°˜í™˜
    if proc.returncode != 0:
        print("âŒ Demucs ì‹¤íŒ¨ â€” ì›ë³¸ìœ¼ë¡œ ì§„í–‰")
        return wav_path

    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ì•ˆì—ì„œ 'vocals.wav' íŒŒì¼ ì°¾ê¸°
    candidates = list(out_root.rglob("vocals.wav"))
    if not candidates: # ê²°ê³¼ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        print("âŒ Demucs ì¶œë ¥ ì—†ìŒ â€” ì›ë³¸ìœ¼ë¡œ ì§„í–‰")
        return wav_path

    demucs_vocals = candidates[0]
    fixed = session_dir / "separation" / "vocals.wav"
    fixed.parent.mkdir(parents=True, exist_ok=True)
    shutil.copyfile(demucs_vocals, fixed)

    # ìµœì¢…ì ìœ¼ë¡œ ë³´ì»¬ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
    return str(fixed)

# ---------------------------
# Whisper ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
# ---------------------------
def whisper_word_timestamps(audio_path: str, language: Optional[str] = "en") -> pd.DataFrame:
    import whisper  # pip install openai-whisper
    print("ğŸ”¤ Whisper(small) ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ...")
    # whisper small ëª¨ë¸ ë¡œë“œ
    model = whisper.load_model("small")
    # ì˜¤ë””ì˜¤ íŒŒì¼ì„ Whisperë¡œ ë³€í™˜
    result = model.transcribe(audio_path, language=language, word_timestamps=True, verbose=False)
    words = [] # ë‹¨ì–´ë³„ ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    for seg in result.get("segments", []):
        for w in seg.get("words", []):
            words.append({
                "start": float(w["start"]),  # ë‹¨ì–´ ì‹œì‘ ì‹œê°„
                "end": float(w["end"]),      # ë‹¨ì–´ ë ì‹œê°„
                "word": w["word"].strip()    # ë‹¨ì–´ í…ìŠ¤íŠ¸
            })
    df = pd.DataFrame(words, columns=["start","end","word"])
    if df.empty:
        raise RuntimeError("Whisperê°€ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return df

# ---------------------------
# í™”ì ë¶„ë¦¬(ë‹¤ì´ì–´ë¼ì´ì œì´ì…˜)
# ---------------------------

@dataclass
class DiarizationSeg:
    start: float  # êµ¬ê°„ ì‹œì‘ ì‹œê°„
    end: float    # êµ¬ê°„ ë ì‹œê°„
    speaker: str  # ì›ë³¸ ë¼ë²¨ (SPEAKER_00 ë“±)

# pyannoteë¥¼ ì´ìš©í•´ ì˜¤ë””ì˜¤ì—ì„œ í™”ì êµ¬ê°„ ì¶”ì¶œ
def run_diarization(audio_path: str, model_id=DIARIZATION_MODEL_ID) -> List[DiarizationSeg]:
    print("ğŸ‘¥ í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘ (pyannote)...")
    from pyannote.audio import Pipeline

    # HuggingFace í† í°ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜
    if not HF_TOKEN or not isinstance(HF_TOKEN, str):
        raise RuntimeError("HF_TOKENì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ HuggingFace í† í°ì„ ë„£ì–´ì£¼ì„¸ìš”.")

    # pyannote diarization íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    pipeline = Pipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN)
    # ì˜¤ë””ì˜¤ íŒŒì¼ì— ëŒ€í•´ í™”ì ë¶„ë¦¬ ìˆ˜í–‰ -> Annotation ê°ì²´ ë°˜í™˜
    diar = pipeline(audio_path)
    segs: List[DiarizationSeg] = []
    for turn, _, speaker in diar.itertracks(yield_label=True):
        segs.append(DiarizationSeg(start=float(turn.start), end=float(turn.end), speaker=str(speaker)))
    # ì‹œì‘ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    segs.sort(key=lambda s: s.start)
    # ì „ì²´ í™”ì ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    uniq = sorted({s.speaker for s in segs})
    print(f"ğŸ§ª unique speakers from diarization: {uniq} (count={len(uniq)})")
    print(f"ğŸ‘¥ í™”ì ì„¸ê·¸ë¨¼íŠ¸ {len(segs)}ê°œ")
    return segs

# DiarizationSeg ë¦¬ìŠ¤íŠ¸ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜
def diar_to_dataframe(segs: List[DiarizationSeg]) -> pd.DataFrame:
    return pd.DataFrame([{"start": s.start, "end": s.end, "speaker": s.speaker} for s in segs])

# í™”ì ë¼ë²¨ì„ "ì‹œê°„ ìˆœì°¨ ë¼ë²¨"ë¡œ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜ -> ìƒˆë¡œìš´ ê°œì„  ë°©ë²•
def _assign_label_seq_over_time(diar_df: pd.DataFrame) -> pd.DataFrame:
    """
    ì‹œê°„ íë¦„ì—ì„œ 'í™”ìê°€ ë°”ë€” ë•Œë§ˆë‹¤' 0,1,2,3...ë¥¼ ë¶€ì—¬.
    ë™ì¼ í™”ìê°€ ë‹¤ì‹œ ë“±ì¥í•´ë„ ìƒˆ ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•˜ëŠ” ê·œì¹™.
    """
    if diar_df.empty:
        return diar_df.copy()
    # ì‹œê°„ ê¸°ì¤€ ì •ë ¬
    diar_df = diar_df.sort_values(["start", "end"]).reset_index(drop=True)
    seq_labels = []
    last_raw = None
    counter = 0
    # ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ìˆœíšŒ
    for _, row in diar_df.iterrows():
        raw = row["speaker"]
        # ì§ì „ í™”ìì™€ ë‹¤ë¥´ë©´ ìƒˆë¡œìš´ ë²ˆí˜¸ ë¶€ì—¬
        if raw != last_raw:
            label = str(counter)
            counter += 1
            last_raw = raw
        seq_labels.append(label)
    # speaker_seq ì»¬ëŸ¼ ì¶”ê°€
    out = diar_df.copy()
    out["speaker_seq"] = seq_labels
    return out

# ì‹¤ì œë¡œ downstreamì—ì„œ ì‚¬ìš©í•  speaker ì»¬ëŸ¼ ë·° ìƒì„±
def make_active_diar_view(diar_df: pd.DataFrame, use_sequential: bool) -> pd.DataFrame:
    """
    downstreamì—ì„œ ì°¸ì¡°í•  í†µì¼ëœ ì»¬ëŸ¼(speaker)ì„ ìƒì„±í•´ ë°˜í™˜.
    - use_sequential=True  -> speaker <- speaker_seq
    - use_sequential=False -> speaker ê·¸ëŒ€ë¡œ
    """
    if diar_df.empty:
        return diar_df.copy()
    if use_sequential:
        if "speaker_seq" not in diar_df.columns:
            diar_df = _assign_label_seq_over_time(diar_df)
        # speaker_seqë¥¼ speakerë¡œ ë°”ê¿”ì„œ ë°˜í™˜
        view = diar_df[["start", "end", "speaker_seq"]].rename(columns={"speaker_seq": "speaker"}).copy()
    else:
        # ì›ë³¸ ë¼ë²¨ ê·¸ëŒ€ë¡œ ë°˜í™˜
        view = diar_df[["start", "end", "speaker"]].copy()
    return view

# íŠ¹ì • ì‹œê° tì—ì„œ í™œì„± í™”ì ë¼ë²¨ ì°¾ê¸°
def assign_speaker_at_time(t: float, diar_view: pd.DataFrame, default="unknown") -> str:
    # tê°€ ì†í•œ í™”ì êµ¬ê°„ ì°¾ê¸°
    hits = diar_view[(diar_view["start"] <= t) & (diar_view["end"] >= t)]
    if hits.empty:
        return default
    # ê²¹ì¹˜ëŠ” êµ¬ê°„ì´ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°, ê¸¸ì´ê°€ ê°€ì¥ ê¸´ êµ¬ê°„ ì„ íƒ
    lens = (hits["end"] - hits["start"]).to_numpy()
    return str(hits.iloc[int(lens.argmax())]["speaker"])

# í”„ë ˆì„ ë‹¨ìœ„ë¡œ í™”ì ë¼ë²¨ ë§¤í•‘
def assign_speaker_to_frames(frame_times: np.ndarray, diar_view: pd.DataFrame) -> List[str]:
    return [assign_speaker_at_time(float(t), diar_view) for t in frame_times]

# ë‹¨ì–´ë³„ë¡œ í™”ì ë¼ë²¨ ë¶€ì—¬
def annotate_words_with_speaker(words_df: pd.DataFrame, diar_view: pd.DataFrame) -> pd.DataFrame:
    speakers = []
    for _, w in words_df.iterrows():
        # ë‹¨ì–´ì˜ ì¤‘ê°„ ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ í™”ì ì°¾ê¸°
        mid = (float(w["start"]) + float(w["end"])) / 2.0
        speakers.append(assign_speaker_at_time(mid, diar_view))
    out = words_df.copy()
    out["speaker"] = speakers
    return out

# ---------------------------
# í”„ë ˆì„ í”¼ì²˜ë§
# ---------------------------
@dataclass
class EmotionFrames:
    times: np.ndarray   # ê° í”„ë ˆì„ ì¤‘ì•™ ì‹œê°„
    probs: np.ndarray   # ê° í”„ë ˆì„ì˜ ê°ì • í™•ë¥  (N, C)
    rms: np.ndarray     # í”„ë ˆì„ë³„ RMS ê°€ì¤‘ì¹˜
    labels: List[str]   # ê°ì • í´ë˜ìŠ¤ëª… ë¦¬ìŠ¤íŠ¸

# ì˜¤ë””ì˜¤ë¥¼ win_s, hop_s ë‹¨ìœ„ë¡œ í”„ë ˆì„ ë¶„í• 
def frame_audio(y: np.ndarray, sr: int, win_s=WIN_S, hop_s=HOP_S):
    win = int(win_s * sr)
    hop = int(hop_s * sr)
    frames, centers = [], []
    for start in range(0, max(1, len(y) - win + 1), hop):
        end = start + win
        if end > len(y):
            break
        frames.append(y[start:end])
        centers.append((start + end) / 2 / sr)
    frames = np.stack(frames) if frames else np.empty((0,))
    centers = np.array(centers, dtype=float)
    rms = rms_per_frame(frames)
    return frames, centers, rms

# feature extractorì—ì„œ ìƒ˜í”Œë§ë ˆì´íŠ¸ ì–»ê¸°
def _get_sampling_rate(fe) -> int:
    fe_like = getattr(fe, "feature_extractor", fe)
    return getattr(fe_like, "sampling_rate", 16000)

# ---------------------------
# ê°ì • ë¶„ë¥˜ ëª¨ë¸
# ---------------------------

# ê°ì • ëª¨ë¸ì„ ì‚¬ìš©í•´ í”„ë ˆì„ë³„ ê°ì • í™•ë¥  ê³„ì‚°
def emotion_frame_probs(y: np.ndarray, sr: int, model_id=EMO_MODEL_ID) -> EmotionFrames:
    # HuggingFaceì—ì„œ ëª¨ë¸ê³¼ feature extractor ë¡œë“œ
    fe = AutoFeatureExtractor.from_pretrained(model_id)
    model = AutoModelForAudioClassification.from_pretrained(model_id)
    model.eval()

    target_sr = _get_sampling_rate(fe)
    required_len = int(target_sr * 30.0)  # Whisper ì…ë ¥ ê³ ì • 30ì´ˆ
    # ì˜¤ë””ì˜¤ë¥¼ mono & target_srë¡œ ë³€í™˜
    y, sr = ensure_mono_sr(y, sr, target_sr=target_sr)

    # ì˜¤ë””ì˜¤ë¥¼ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë¶„í• 
    frames, centers, rms = frame_audio(y, sr, WIN_S, HOP_S)

    # ë§Œì•½ í”„ë ˆì„ì´ ì—†ìœ¼ë©´ ë¹ˆ EmotionFrames ë°˜í™˜
    if frames.size == 0:
        num_labels = getattr(model.config, "num_labels", 0)
        return EmotionFrames(np.array([]), np.zeros((0, num_labels)), np.array([]), [])

    # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ cuda, ì•„ë‹ˆë©´ cpu ì„ íƒ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    all_probs = []
    BS = 64

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ë“  í”„ë ˆì„ ì² 
    for i in range(0, len(frames), BS):
        batch_raw = frames[i:i+BS]

        # Whisper ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ 30ì´ˆ ê¸¸ì´ë¡œ íŒ¨ë”©
        padded = []
        for f in batch_raw:
            f = np.asarray(f, dtype=np.float32)
            if len(f) >= required_len:
                padded.append(f[:required_len])
            else:
                pad = np.zeros(required_len, dtype=np.float32)
                pad[:len(f)] = f
                padded.append(pad)

        # feature extractor ì ìš© (í…ì„œ ë³€í™˜)
        inputs = fe(padded, sampling_rate=sr, return_tensors="pt")

        if isinstance(inputs, dict) and "attention_mask" in inputs:
            inputs.pop("attention_mask", None)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            logits = model(**inputs).logits
            # softmax -> ê°ì • í™•ë¥ 
            probs = F.softmax(logits, dim=-1).detach().cpu().numpy()
            all_probs.append(probs)

    # ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ í•©ì¹˜ê¸°
    probs = np.concatenate(all_probs, axis=0)
    # ëª¨ë¸ ì„¤ì •ì—ì„œ í´ë˜ìŠ¤ ì´ë¦„ ë¶ˆëŸ¬ì˜¤ê¸°
    labels = [model.config.id2label[i] for i in range(probs.shape[1])]
    # EmotionFrames ê°ì²´ ë°˜í™˜
    return EmotionFrames(centers, probs, rms, labels)

# ---------------------------
# í”¼ì¹˜: F0 -> ì„¸ë¯¸í†¤ -> ì†Œí”„íŠ¸ í™•ë¥  (í™”ìë³„ ê¸°ì¤€ì¹˜)
# ---------------------------

# í”¼ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ìš© ë°ì´í„° í´ë˜ìŠ¤
@dataclass
class PitchFrames:
    times: np.ndarray           # ê° í”„ë ˆì„ì˜ ì¤‘ì•™ ì‹œê°„
    probs: np.ndarray           # ê° í”„ë ˆì„ì˜ í”¼ì¹˜ softmax í™•ë¥  (N x 3 : [low, mid, high])
    entropy: np.ndarray         # ê° í”„ë ˆì„ì˜ í™•ë¥  ì—”íŠ¸ë¡œí”¼ (ë¶ˆí™•ì‹¤ì„± ì§€í‘œ)
    f0_med_per_speaker: dict    # í™”ìë³„ F0 ì¤‘ì•™ê°’ {speaker: median_f0}
    st_values: np.ndarray       # ê° í”„ë ˆì„ì˜ ì„¸ë¯¸í†¤ ê°’ (ìƒëŒ€ í”¼ì¹˜)
    frame_speakers: List[str]   # ê° í”„ë ˆì„ì— í• ë‹¹ëœ í™”ì

# ê° í”„ë ˆì„ë³„ ê¸°ë³¸ ì£¼íŒŒìˆ˜(F0) ì¶”ì •
def estimate_f0_per_frame(frames: np.ndarray, sr: int) -> np.ndarray:
    f0_all = []
    for f in frames:
        try:
            # librosa.pyin: F0 ì¶”ì • (ë²”ìœ„: 50Hz ~ 600Hz)
            f0, _, _ = librosa.pyin(
                f, fmin=50, fmax=600, sr=sr,
                frame_length=2048, hop_length=512
            )
            f0_val = np.nanmedian(f0)  # NaN ì œì™¸í•œ ì¤‘ì•™ê°’
        except Exception:
            f0_val = np.nan
        # ê°’ì´ ìˆìœ¼ë©´ [50, 600] ë²”ìœ„ë¡œ clip
        if not np.isnan(f0_val):
            f0_val = float(np.clip(f0_val, 50, 600))
        f0_all.append(f0_val)
    return np.array(f0_all, dtype=float)

# í”¼ì¹˜ ì†Œí”„íŠ¸ í™•ë¥  ê³„ì‚° (í™”ìë³„ ê¸°ì¤€ F0 ë°˜ì˜)
def pitch_soft_probs_with_speakers(
    y: np.ndarray, sr: int,
    diar_view: Optional[pd.DataFrame] = None,   # í™”ì ì •ë³´ (start, end, speaker)
    win_s=WIN_S, hop_s=HOP_S,                   # í”„ë ˆì„ ê¸¸ì´/í™‰ (ì´ˆ)
    delta_st=PITCH_DELTA_ST, sigma=PITCH_SIGMA  # ë¶„ë¥˜ ê²½ê³„ ë° ë¶„ì‚°
) -> PitchFrames:
    # ì˜¤ë””ì˜¤ë¥¼ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë¶„í• 
    frames, centers, _ = frame_audio(y, sr, win_s, hop_s)
    if frames.size == 0:
        # í”„ë ˆì„ì´ ì—†ì„ ê²½ìš° ë¹ˆ êµ¬ì¡° ë°˜í™˜
        return PitchFrames(np.array([]), np.zeros((0,3)), np.array([]), {}, np.array([]), [])

    # ê° í”„ë ˆì„ë³„ F0 ì¶”ì •
    f0_all = estimate_f0_per_frame(frames, sr)

    # í”„ë ˆì„ë³„ í™”ì í• ë‹¹ (ë‹¤ì´ì–´ë¦¬ì œì´ì…˜ ê²°ê³¼ í™œìš©)
    if diar_view is not None and not diar_view.empty:
        frame_speakers = assign_speaker_to_frames(centers, diar_view)
    else:
        frame_speakers = ["unknown"] * len(centers)

    # í™”ìë³„ F0 ì¤‘ì•™ê°’ ê³„ì‚°
    f0_med_per_speaker = {}
    fs = np.array(frame_speakers, dtype=object)
    for spk in set(frame_speakers):
        vals = f0_all[(fs == spk) & (~np.isnan(f0_all))]
        if len(vals) > 0:
            f0_med_per_speaker[spk] = float(np.median(vals))

    # ëª¨ë“  í™”ìë¥¼ í†µí‹€ì–´ global median (ë°±ì—…ìš©)
    global_med = float(np.median(f0_all[~np.isnan(f0_all)])) if np.any(~np.isnan(f0_all)) else np.nan

    # ê° í”„ë ˆì„ì˜ ìƒëŒ€ ì„¸ë¯¸í†¤ ê³„ì‚°
    st = np.zeros_like(centers, dtype=float)
    for i, (f0, spk) in enumerate(zip(f0_all, frame_speakers)):
        base = f0_med_per_speaker.get(spk, global_med)  # í™”ìë³„ ì¤‘ì•™ê°’ ê¸°ì¤€
        if np.isnan(f0) or not np.isfinite(base):
            st[i] = 0.0  # ê°’ì´ ì—†ìœ¼ë©´ 0
        else:
            # ì„¸ë¯¸í†¤ ë³€í™˜: 12 * log2(f0 / ê¸°ì¤€ê°’)
            st[i] = 12.0 * np.log2(max(f0, 1e-6) / base)

    # ì†Œí”„íŠ¸ ë¶„ë¥˜ (low, mid, high)
    # ì¤‘ì‹¬ì : -delta_st, 0, +delta_st
    centers_st = np.array([-delta_st, 0.0, +delta_st])[:, None]

    # ê° ì„¸ë¯¸í†¤ ê°’ì— ëŒ€í•´ ê°€ìš°ì‹œì•ˆ ë¶„í¬ ì ìˆ˜ ê³„ì‚°
    scores = np.exp(-0.5 * ((st[None, :] - centers_st) / sigma) ** 2)

    # ì •ê·œí™”í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜ (softmaxì™€ ìœ ì‚¬)
    probs = (scores / (scores.sum(axis=0, keepdims=True) + 1e-9)).T

    # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ë¶ˆí™•ì‹¤ì„± ì •ë„)
    ent = -np.sum(probs * np.log(probs + 1e-9), axis=1)

    # ìµœì¢… ê²°ê³¼ êµ¬ì¡°ì²´ ë°˜í™˜
    return PitchFrames(centers, probs, ent, f0_med_per_speaker, st, frame_speakers)

# ---------------------------
# ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„ â€” í™”ìê°’ 'ê·¸ëŒ€ë¡œ' ë³´ì¡´(ìˆœì°¨ ë¼ë²¨)
# ---------------------------
def aggregate_over_words(words_seq_df: pd.DataFrame,
                         emo: EmotionFrames,
                         pitch: PitchFrames,
                         pad_s: float = 0.05) -> pd.DataFrame:
    out_rows = []
    emo_labels = emo.labels if emo.labels else [f"class_{i}" for i in range(emo.probs.shape[1])]
    pitch_labels = ["low", "mid", "high"]

    etimes, eprobs, erms = emo.times, emo.probs, emo.rms
    ptimes, pprobs = pitch.times, pitch.probs

    for _, w in words_seq_df.iterrows():
        t0 = float(w["start"]) - pad_s
        t1 = float(w["end"]) + pad_s

        # ğŸ”’ í™”ì: ìˆœì°¨ ë¼ë²¨ ì»¬ëŸ¼ speakerë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        speaker_seq = str(w["speaker"]) if "speaker" in w and pd.notna(w["speaker"]) else "unknown"

        # ê°ì • ì§‘ê³„ (RMS ê°€ì¤‘ í‰ê· )
        if etimes.size:
            mask_e = (etimes >= t0) & (etimes <= t1)
            Pe = eprobs[mask_e]
            We = erms[mask_e]
            if Pe.shape[0] > 0:
                if We.sum() <= 1e-12:
                    We = np.ones_like(We) / len(We)
                else:
                    We = We / We.sum()
                emo_mean = (Pe * We[:, None]).sum(axis=0)
                emo_entropy = float(-(emo_mean * np.log(emo_mean + 1e-9)).sum())
                emo_top = emo_labels[int(np.argmax(emo_mean))]
                emo_probs_dict = {emo_labels[i]: float(emo_mean[i]) for i in range(len(emo_mean))}
            else:
                emo_entropy = float(np.nan); emo_top = ""; emo_probs_dict = {}
        else:
            emo_entropy = float(np.nan); emo_top = ""; emo_probs_dict = {}

        # í”¼ì¹˜ ì§‘ê³„ (í‰ê· )
        if ptimes.size:
            mask_p = (ptimes >= t0) & (ptimes <= t1)
            Pp = pprobs[mask_p]
            if Pp.shape[0] > 0:
                Wp = np.ones((Pp.shape[0],), dtype=float) / Pp.shape[0]
                pitch_mean = (Pp * Wp[:, None]).sum(axis=0)
                pitch_entropy = float(-(pitch_mean * np.log(pitch_mean + 1e-9)).sum())
                pitch_top = pitch_labels[int(np.argmax(pitch_mean))]
                pitch_probs_dict = {pitch_labels[i]: float(pitch_mean[i]) for i in range(3)}
            else:
                pitch_entropy = float(np.nan); pitch_top = ""; pitch_probs_dict = {}
        else:
            pitch_entropy = float(np.nan); pitch_top = ""; pitch_probs_dict = {}

        out_rows.append({
            "start": float(w["start"]),
            "end": float(w["end"]),
            "word": str(w["word"]),
            "speaker": speaker_seq,                      # â† ìˆœì°¨ ë¼ë²¨
            "emo_label": emo_top,
            "emo_entropy": emo_entropy,
            "emo_probs": json.dumps(emo_probs_dict, ensure_ascii=False),
            "pitch_label": pitch_top,
            "pitch_entropy": pitch_entropy,
            "pitch_probs": json.dumps(pitch_probs_dict, ensure_ascii=False),
        })

    return pd.DataFrame(out_rows)

# ---------------------------
# ë©”ì¸
# ---------------------------
def main():
    safe_mkdir(OUTPUT_DIR)

    # 1) ì…ë ¥ ë¡œë“œ & ë³´ì»¬ ë¶„ë¦¬(ì„ íƒ)
    src_wav = INPUT_AUDIO
    print(f"ğŸµ Input: {src_wav}")
    use_path = separate_vocals_with_demucs(src_wav, OUTPUT_DIR) if USE_VOCAL_SEPARATION else src_wav
    print(f"ğŸ¤ ë¶„ì„ ì˜¤ë””ì˜¤: {use_path}")

    # 2) ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„
    if USE_WHISPER:
        try:
            words_df = whisper_word_timestamps(use_path, language=LANG)
        except Exception as e:
            print(f"âš ï¸ Whisper ì‹¤íŒ¨: {e}")
            if WORDS_CSV and os.path.exists(WORDS_CSV):
                words_df = pd.read_csv(WORDS_CSV)
                assert {"start","end","word"}.issubset(words_df.columns)
            else:
                raise
    else:
        if WORDS_CSV and os.path.exists(WORDS_CSV):
            words_df = pd.read_csv(WORDS_CSV)
            assert {"start","end","word"}.issubset(words_df.columns)
        else:
            raise RuntimeError("USE_WHISPER=False ì¸ ê²½ìš° WORDS_CSV ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    words_raw_csv = os.path.join(OUTPUT_DIR, "words_raw.csv")
    write_csv(words_df, words_raw_csv)
    preview(words_df, "Whisper ë‹¨ì–´")
    print(f"ğŸ“„ ë‹¨ì–´ íŒŒì¼ ì €ì¥: {words_raw_csv}")

    # 3) í™”ì ë¶„ë¦¬(ì„ íƒ) -> "í™œì„± ë‹¤ì´ì–´ë¦¬ë·°" ìƒì„± (ìˆœì°¨ ë ˆì´ë¸” ëª¨ë“œì¼ ê²½ìš° speaker_seq ì‚¬ìš©)
    diar_df = pd.DataFrame(columns=["start","end","speaker"])
    diar_view = pd.DataFrame(columns=["start","end","speaker"])
    if USE_DIARIZATION:
        try:
            segs = run_diarization(use_path, DIARIZATION_MODEL_ID)
            diar_df = diar_to_dataframe(segs)
            # ìˆœì°¨ ë ˆì´ë¸” ì»¬ëŸ¼ ì¶”ê°€
            diar_df = _assign_label_seq_over_time(diar_df)
            diar_csv = os.path.join(OUTPUT_DIR, "diarization_segments.csv")
            write_csv(diar_df, diar_csv)

            # ğŸ’¡ downstreamì—ì„œ ì°¸ì¡°í•  'speaker' ë·° í™•ì •
            diar_view = make_active_diar_view(diar_df, use_sequential=SEQUENTIAL_SPEAKER_LABELS)

            # ë‹¨ì–´ì— í™”ì ë¶€ì—¬ (í™œì„± ë·° ê¸°ì¤€)
            words_df = annotate_words_with_speaker(words_df, diar_view)
        except Exception as e:
            print(f"âš ï¸ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")
            diar_df = pd.DataFrame(columns=["start","end","speaker"])
            diar_view = diar_df.copy()
            words_df["speaker"] = "unknown"
    else:
        words_df["speaker"] = "unknown"

    # ğŸ”’ words_dfì— ê¸°ë¡ëœ speaker ë¥¼ ê³ ì •(ì´ ê°’ì´ words_with_speaker.csv & ìµœì¢… CSVì— ê·¸ëŒ€ë¡œ ì‚¬ìš©ë¨)
    words_df = words_df.sort_values(["start","end"]).reset_index(drop=True)

    # words_with_speaker.csv ì €ì¥
    words_spk_csv = os.path.join(OUTPUT_DIR, "words_with_speaker.csv")
    write_csv(words_df, words_spk_csv)
    preview(words_df, "ë‹¨ì–´+í™”ì ë¼ë²¨(ìµœì¢… ê³ ì •)")
    print(f"ğŸ“„ ë‹¨ì–´+í™”ì íŒŒì¼ ì €ì¥: {words_spk_csv}")


    # 4) ì˜¤ë””ì˜¤ ë¡œë“œ (ê°ì •/í”¼ì¹˜ ì¶”ë¡  ì¤€ë¹„)
    y, sr = load_audio(use_path)

    # 5) ê°ì • í”„ë ˆì„ ì¶”ë¡  (ê¸°ì¡´ ë¡œì§)
    print("ğŸ§  Emotion frame inference...")
    emo = emotion_frame_probs(y, sr, EMO_MODEL_ID)

    # 6) í”¼ì¹˜: âœ… "í™œì„± ë‹¤ì´ì–´ë¦¬ë·°(speaker=ìˆœì°¨ ë ˆì´ë¸”)" ê¸°ì¤€ìœ¼ë¡œ í™”ìë³„ F0 ì¤‘ì•™ê°’ ê³„ì‚°
    print("ğŸ¼ Pitch soft-prob (speaker-aware, sequential labels if enabled)...")
    pitch = pitch_soft_probs_with_speakers(y, sr, diar_view, WIN_S, HOP_S, PITCH_DELTA_ST, PITCH_SIGMA)

    # 7) ë‹¨ì–´ êµ¬ê°„ ì§‘ê³„ (words_dfì˜ speaker ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    print("ğŸ§® Aggregating per word...")
    out_df = aggregate_over_words(words_df, emo, pitch)

    # 8) ì €ì¥
    out_csv = os.path.join(OUTPUT_DIR, "words_emotion_pitch.csv")
    write_csv(out_df, out_csv)
    print("âœ… Done.")

if __name__ == "__main__":
    main()
